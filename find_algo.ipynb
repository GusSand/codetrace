{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation patching basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cuda:3\"\n",
    "starcoderbase_1b = \"/home/arjun/models/starcoderbase-1b/\"\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from src.utils import *\n",
    "import datasets\n",
    "from src.experiments.wrong_type_patching import *\n",
    "from src.experiments.type_patching import *\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import plotly.express as px\n",
    "from src.patching import *\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from src.utils import *\n",
    "import regex as re\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GPTBigCodeForCausalLM(\n",
       "   (transformer): GPTBigCodeModel(\n",
       "     (wte): Embedding(49152, 2048)\n",
       "     (wpe): Embedding(8192, 2048)\n",
       "     (drop): Dropout(p=0.1, inplace=False)\n",
       "     (h): ModuleList(\n",
       "       (0-23): 24 x GPTBigCodeBlock(\n",
       "         (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPTBigCodeSdpaAttention(\n",
       "           (c_attn): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "           (c_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPTBigCodeMLP(\n",
       "           (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "           (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "           (act): PytorchGELUTanh()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       " ),\n",
       " GPTBigCodeConfig {\n",
       "   \"_name_or_path\": \"/home/arjun/models/starcoderbase-1b/\",\n",
       "   \"activation_function\": \"gelu_pytorch_tanh\",\n",
       "   \"architectures\": [\n",
       "     \"GPTBigCodeForCausalLM\"\n",
       "   ],\n",
       "   \"attention_softmax_in_fp32\": true,\n",
       "   \"attn_pdrop\": 0.1,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"embd_pdrop\": 0.1,\n",
       "   \"eos_token_id\": 0,\n",
       "   \"inference_runner\": 0,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"max_batch_size\": null,\n",
       "   \"max_sequence_length\": null,\n",
       "   \"model_type\": \"gpt_bigcode\",\n",
       "   \"multi_query\": true,\n",
       "   \"n_embd\": 2048,\n",
       "   \"n_head\": 16,\n",
       "   \"n_inner\": 8192,\n",
       "   \"n_layer\": 24,\n",
       "   \"n_positions\": 8192,\n",
       "   \"pad_key_length\": true,\n",
       "   \"pre_allocate_kv_cache\": false,\n",
       "   \"resid_pdrop\": 0.1,\n",
       "   \"scale_attention_softmax_in_fp32\": true,\n",
       "   \"scale_attn_weights\": true,\n",
       "   \"summary_activation\": null,\n",
       "   \"summary_first_dropout\": 0.1,\n",
       "   \"summary_proj_to_labels\": true,\n",
       "   \"summary_type\": \"cls_index\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"torch_dtype\": \"float32\",\n",
       "   \"transformers_version\": \"4.37.0\",\n",
       "   \"use_cache\": true,\n",
       "   \"validate_runner_input\": true,\n",
       "   \"vocab_size\": 49152\n",
       " })"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(starcoderbase_1b, device_map=device)\n",
    "model, model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating what happens when a model gives the wrong prediction\n",
    "\n",
    "Here we give an approximate algorithm of what the model may be doing:\n",
    "\n",
    "1. identify first untyped variable and bind it to a value of the same type by transfering info from value to variable\n",
    "2. repeat 1 until all variables are typed\n",
    "3. copy the answer onto `<fim_middle>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt', 'solution', 'generated', 'success', 'filename', 'variation_idx'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'solution', 'generated', 'success', 'filename', 'variation_idx'],\n",
       "     num_rows: 163\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'solution', 'generated', 'success', 'filename', 'variation_idx'],\n",
       "     num_rows: 137\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"franlucc/starcoderbase-1b-wrong-type-patching-v0-seed42\", split=\"train\")\n",
    "working_ds = ds.filter(lambda x: x[\"success\"])\n",
    "broken_ds = ds.filter(lambda x: not x[\"success\"])\n",
    "ds, working_ds, broken_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at one failing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_uniq_0 \n",
      "\n",
      " _uniq_1) => _uniq_0,\n",
      "  _uniq_11: _uniq_0\n",
      "): _uniq_0 {\n",
      "  switch (_uniq_9.kind) {\n",
      "    case \"Ctor4\": {\n",
      "      return _uniq\n",
      "SOLUTION: _uniq_q \n",
      "\n",
      " GENERATED: _uniq_t => _uniq_q,\n",
      "  _uniq_11: _uniq_q\n",
      "): _uniq_q {\n",
      "  switch (_uniq_9.kind) {\n",
      "    case \"Ctor4\": {\n",
      "      return _uniq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kr\">declare</span><span class=\"w\"> </span><span class=\"kd\">var</span><span class=\"w\"> </span><span class=\"nx\">require</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">any</span><span class=\"p\">;</span>\n",
       "<span class=\"kd\">const</span><span class=\"w\"> </span><span class=\"nx\">assert</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">require</span><span class=\"p\">(</span><span class=\"s2\">&quot;node:assert&quot;</span><span class=\"p\">);</span>\n",
       "<span class=\"kr\">type</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor2&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor3&quot;</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f0</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"w\"> </span><span class=\"p\">};</span>\n",
       "<span class=\"kr\">type</span><span class=\"w\"> </span><span class=\"nx\">_uniq_t</span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor4&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor5&quot;</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f0</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f1</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_t</span><span class=\"p\">};</span>\n",
       "\n",
       "<span class=\"kd\">function</span><span class=\"w\"> </span><span class=\"nx\">_uniq_6</span><span class=\"p\">(</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_9</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_t</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_10</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nx\">__x7</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">__x8</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"nx\">FILL</span><span class=\"o\">&gt;</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_11</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span>\n",
       "<span class=\"p\">)</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">  </span><span class=\"k\">switch</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">kind</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"k\">case</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor4&quot;</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">      </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nx\">_uniq_11</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"k\">case</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor5&quot;</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">      </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"nx\">_uniq_13</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">f1</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">      </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"nx\">_uniq_12</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">f0</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">      </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nx\">_uniq_6</span><span class=\"p\">(</span><span class=\"nx\">_uniq_13</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_10</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_10</span><span class=\"p\">(</span><span class=\"nx\">_uniq_11</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_12</span><span class=\"p\">));</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">  </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kr}{declare}\\PY{+w}{ }\\PY{k+kd}{var}\\PY{+w}{ }\\PY{n+nx}{require}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{any}\\PY{p}{;}\n",
       "\\PY{k+kd}{const}\\PY{+w}{ }\\PY{n+nx}{assert}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{require}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}node:assert\\PYZdq{}}\\PY{p}{)}\\PY{p}{;}\n",
       "\\PY{k+kr}{type}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor2\\PYZdq{}}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{+w}{ }\\PY{o}{|}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor3\\PYZdq{}}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f0}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{p}{;}\n",
       "\\PY{k+kr}{type}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}t}\\PY{o}{=}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor4\\PYZdq{}}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{+w}{ }\\PY{o}{|}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor5\\PYZdq{}}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f0}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f1}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}t}\\PY{p}{\\PYZcb{}}\\PY{p}{;}\n",
       "\n",
       "\\PY{k+kd}{function}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}6}\\PY{p}{(}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}t}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{(}\\PY{n+nx}{\\PYZus{}\\PYZus{}x7}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}\\PYZus{}x8}\\PY{o}{:}\\PY{+w}{ }\\PY{o}{\\PYZlt{}}\\PY{n+nx}{FILL}\\PY{o}{\\PYZgt{}}\\PY{p}{)}\\PY{+w}{ }\\PY{p}{=\\PYZgt{}}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{,}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\n",
       "\\PY{p}{)}\\PY{o}{:}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{  }\\PY{k}{switch}\\PY{+w}{ }\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{kind}\\PY{p}{)}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{k}{case}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor4\\PYZdq{}}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{      }\\PY{k}{return}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{p}{;}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{k}{case}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor5\\PYZdq{}}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{      }\\PY{k+kd}{let}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}13}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{f1}\\PY{p}{;}\n",
       "\\PY{+w}{      }\\PY{k+kd}{let}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}12}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{f0}\\PY{p}{;}\n",
       "\\PY{+w}{      }\\PY{k}{return}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}6}\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}13}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}12}\\PY{p}{)}\\PY{p}{)}\\PY{p}{;}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{  }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "declare var require: any;\n",
       "const assert = require(\"node:assert\");\n",
       "type _uniq_q = { kind: \"Ctor2\" } | { kind: \"Ctor3\"; f0: _uniq_q };\n",
       "type _uniq_t= { kind: \"Ctor4\" } | { kind: \"Ctor5\"; f0: _uniq_q; f1: _uniq_t};\n",
       "\n",
       "function _uniq_6(\n",
       "  _uniq_9: _uniq_t\n",
       "  _uniq_10: (__x7: _uniq_q, __x8: <FILL>) => _uniq_q,\n",
       "  _uniq_11: _uniq_q\n",
       "): _uniq_q {\n",
       "  switch (_uniq_9.kind) {\n",
       "    case \"Ctor4\": {\n",
       "      return _uniq_11;\n",
       "    }\n",
       "    case \"Ctor5\": {\n",
       "      let _uniq_13 = _uniq_9.f1;\n",
       "      let _uniq_12 = _uniq_9.f0;\n",
       "      return _uniq_6(_uniq_13, _uniq_10, _uniq_10(_uniq_11, _uniq_12));\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Code\n",
    "content = broken_ds[0][\"prompt\"]\n",
    "solution = broken_ds[0][\"solution\"]\n",
    "generated = broken_ds[0][\"generated\"]\n",
    "print(solution, \"\\n\\n\", generated)\n",
    "content = content.split(\"function _uniq_7\")[0]\n",
    "\n",
    "rename = {\n",
    "    \"_uniq_0\": \"_uniq_q\",\n",
    "    \"_uniq_1[^\\d]\": \"_uniq_t\",\n",
    "}\n",
    "def rename_vars(content, rename):\n",
    "    for old, new in rename.items():\n",
    "        content = re.sub(old, new, content)\n",
    "    return content\n",
    "\n",
    "content = rename_vars(content, rename)\n",
    "solution = rename_vars(solution, rename)\n",
    "generated = rename_vars(generated, rename)\n",
    "print(\"SOLUTION:\", solution, \"\\n\\n\", \"GENERATED:\", generated)\n",
    "\n",
    "code = Code(content, language=\"typescript\")\n",
    "base_prompt = placeholder_to_std_fmt(content, STARCODER_FIM)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 172, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"function\" +base_prompt.split(\"function\")[-1]\n",
    "\n",
    "max_new_tokens = 4\n",
    "\n",
    "def get_final_states(prompt, max_new_tokens):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            hidden_states = []\n",
    "            for i in range(max_new_tokens):\n",
    "                final_hs = model.lm_head.output\n",
    "                print(model.transformer.h[5].mlp.output.shape)\n",
    "                hidden_states.append(final_hs.save())\n",
    "                invoker.next()\n",
    "    return hidden_states\n",
    "\n",
    "hidden_states = get_final_states(prompt, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_uniq_q\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_pred_0</th>\n",
       "      <th>top_pred_1</th>\n",
       "      <th>top_pred_2</th>\n",
       "      <th>top_pred_3</th>\n",
       "      <th>top_pred_4</th>\n",
       "      <th>top_pred_5</th>\n",
       "      <th>top_pred_6</th>\n",
       "      <th>top_pred_7</th>\n",
       "      <th>top_pred_8</th>\n",
       "      <th>top_pred_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>next_token_0</th>\n",
       "      <td>(_, 0.8836920261383057)</td>\n",
       "      <td>(__, 0.04612018167972565)</td>\n",
       "      <td>(string, 0.01082543283700943)</td>\n",
       "      <td>((, 0.009049000218510628)</td>\n",
       "      <td>((_, 0.006661245133727789)</td>\n",
       "      <td>(number, 0.005222067702561617)</td>\n",
       "      <td>(any, 0.002787975361570716)</td>\n",
       "      <td>((__, 0.0024021132849156857)</td>\n",
       "      <td>(&lt;|endoftext|&gt;, 0.0022207931615412235)</td>\n",
       "      <td>(\", 0.0017489035381004214)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_token_1</th>\n",
       "      <td>(uniq, 0.9975231289863586)</td>\n",
       "      <td>(q, 0.0001862113713286817)</td>\n",
       "      <td>(t, 9.274380136048421e-05)</td>\n",
       "      <td>(a, 9.199933265335858e-05)</td>\n",
       "      <td>(uni, 9.078498260350898e-05)</td>\n",
       "      <td>(distinct, 8.081931446213275e-05)</td>\n",
       "      <td>(unique, 7.646429730812088e-05)</td>\n",
       "      <td>(u, 4.0061382605927065e-05)</td>\n",
       "      <td>(x, 3.667894270620309e-05)</td>\n",
       "      <td>(Q, 3.614146771724336e-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_token_2</th>\n",
       "      <td>(_, 0.9995818734169006)</td>\n",
       "      <td>(__, 0.0001161865220637992)</td>\n",
       "      <td>(2, 3.273446054663509e-05)</td>\n",
       "      <td>(&lt;|endoftext|&gt;, 2.614452205307316e-05)</td>\n",
       "      <td>(q, 1.9497674657031894e-05)</td>\n",
       "      <td>(), 1.9011838958249427e-05)</td>\n",
       "      <td>(_), 1.5274810721166432e-05)</td>\n",
       "      <td>(1, 1.4974726582295261e-05)</td>\n",
       "      <td>(,, 1.312915173912188e-05)</td>\n",
       "      <td>(., 1.0743523489509244e-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_token_3</th>\n",
       "      <td>(q, 0.6972823143005371)</td>\n",
       "      <td>(t, 0.1781110316514969)</td>\n",
       "      <td>(p, 0.02561504766345024)</td>\n",
       "      <td>(r, 0.014788011088967323)</td>\n",
       "      <td>(s, 0.00788042787462473)</td>\n",
       "      <td>(1, 0.007198556326329708)</td>\n",
       "      <td>(a, 0.006413357798010111)</td>\n",
       "      <td>(w, 0.003968821372836828)</td>\n",
       "      <td>(u, 0.0035089619923382998)</td>\n",
       "      <td>(b, 0.003295798785984516)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              top_pred_0                   top_pred_1  \\\n",
       "next_token_0     (_, 0.8836920261383057)    (__, 0.04612018167972565)   \n",
       "next_token_1  (uniq, 0.9975231289863586)   (q, 0.0001862113713286817)   \n",
       "next_token_2     (_, 0.9995818734169006)  (__, 0.0001161865220637992)   \n",
       "next_token_3     (q, 0.6972823143005371)      (t, 0.1781110316514969)   \n",
       "\n",
       "                                 top_pred_2  \\\n",
       "next_token_0  (string, 0.01082543283700943)   \n",
       "next_token_1     (t, 9.274380136048421e-05)   \n",
       "next_token_2     (2, 3.273446054663509e-05)   \n",
       "next_token_3       (p, 0.02561504766345024)   \n",
       "\n",
       "                                          top_pred_3  \\\n",
       "next_token_0               ((, 0.009049000218510628)   \n",
       "next_token_1              (a, 9.199933265335858e-05)   \n",
       "next_token_2  (<|endoftext|>, 2.614452205307316e-05)   \n",
       "next_token_3               (r, 0.014788011088967323)   \n",
       "\n",
       "                                top_pred_4                         top_pred_5  \\\n",
       "next_token_0    ((_, 0.006661245133727789)     (number, 0.005222067702561617)   \n",
       "next_token_1  (uni, 9.078498260350898e-05)  (distinct, 8.081931446213275e-05)   \n",
       "next_token_2   (q, 1.9497674657031894e-05)        (), 1.9011838958249427e-05)   \n",
       "next_token_3      (s, 0.00788042787462473)          (1, 0.007198556326329708)   \n",
       "\n",
       "                                   top_pred_6                    top_pred_7  \\\n",
       "next_token_0      (any, 0.002787975361570716)  ((__, 0.0024021132849156857)   \n",
       "next_token_1  (unique, 7.646429730812088e-05)   (u, 4.0061382605927065e-05)   \n",
       "next_token_2     (_), 1.5274810721166432e-05)   (1, 1.4974726582295261e-05)   \n",
       "next_token_3        (a, 0.006413357798010111)     (w, 0.003968821372836828)   \n",
       "\n",
       "                                          top_pred_8  \\\n",
       "next_token_0  (<|endoftext|>, 0.0022207931615412235)   \n",
       "next_token_1              (x, 3.667894270620309e-05)   \n",
       "next_token_2              (,, 1.312915173912188e-05)   \n",
       "next_token_3              (u, 0.0035089619923382998)   \n",
       "\n",
       "                               top_pred_9  \n",
       "next_token_0   (\", 0.0017489035381004214)  \n",
       "next_token_1   (Q, 3.614146771724336e-05)  \n",
       "next_token_2  (., 1.0743523489509244e-05)  \n",
       "next_token_3    (b, 0.003295798785984516)  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states]\n",
    "\n",
    "topk=10\n",
    "predictions = []\n",
    "for j,hs in enumerate(out):\n",
    "    if j == 0:\n",
    "        hs = hs[:, -1, :]\n",
    "        hs = hs.unsqueeze(1)\n",
    "    top_vals_and_idx = hs.softmax(dim=-1).topk(topk)\n",
    "    indices = top_vals_and_idx.indices[0][0].tolist()\n",
    "    values = top_vals_and_idx.values.tolist()[0][0]\n",
    "    top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "    predictions.append(list(zip(top_idx, values)))\n",
    "\n",
    "greedy_prediction = \"\".join([p[0][0] for p in predictions])\n",
    "print(greedy_prediction)\n",
    "result_table = pd.DataFrame(predictions, columns=[f\"top_pred_{i}\" for i in range(topk)], index=[f\"next_token_{i}\" for i in range(max_new_tokens)])\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check generation\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(starcoderbase_1b)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(starcoderbase_1b).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens = 4\n",
    "# tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "# out = llm.generate(tokens, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "# text = [tokenizer.decode(i) for i in out.tolist()]\n",
    "# text = \"\".join(text)\n",
    "# text.split(\"<fim_middle>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<fim_prefix>', 'declare', 'Ġvar', 'Ġrequire', ':', 'Ġany', ';', 'Ċ', 'const', 'Ġassert', 'Ġ=', 'Ġrequire', '(\"', 'node', ':', 'assert', '\");', 'Ċ', 'type', 'Ġ_', 'uniq', '_', 'q', 'Ġ=', 'Ġ{', 'Ġkind', ':', 'Ġ\"', 'Ctor', '2', '\"', 'Ġ}', 'Ġ|', 'Ġ{', 'Ġkind', ':', 'Ġ\"', 'Ctor', '3', '\";', 'Ġf', '0', ':', 'Ġ_', 'uniq', '_', 'q', 'Ġ};', 'Ċ', 'type', 'Ġ_', 'uniq', '_', 't', '=', 'Ġ{', 'Ġkind', ':', 'Ġ\"', 'Ctor', '4', '\"', 'Ġ}', 'Ġ|', 'Ġ{', 'Ġkind', ':', 'Ġ\"', 'Ctor', '5', '\";', 'Ġf', '0', ':', 'Ġ_', 'uniq', '_', 'q', ';', 'Ġf', '1', ':', 'Ġ_', 'uniq', '_', 't', '};', 'Ċ', 'Ċ', 'function', 'Ġ_', 'uniq', '_', '6', '(', 'ĊĠ', 'Ġ_', 'uniq', '_', '9', ':', 'Ġ_', 'uniq', '_', 't', 'ĊĠ', 'Ġ_', 'uniq', '_', '1', '0', ':', 'Ġ(__', 'x', '7', ':', 'Ġ_', 'uniq', '_', 'q', ',', 'Ġ__', 'x', '8', ':', 'Ġ', '<fim_suffix>', ')', 'Ġ=>', 'Ġ_', 'uniq', '_', 'q', ',', 'ĊĠ', 'Ġ_', 'uniq', '_', '1', '1', ':', 'Ġ_', 'uniq', '_', 'q', 'Ċ', '):', 'Ġ_', 'uniq', '_', 'q', 'Ġ{', 'ĊĠ', 'Ġswitch', 'Ġ(_', 'uniq', '_', '9', '.', 'kind', ')', 'Ġ{', 'ĊĠĠĠ', 'Ġcase', 'Ġ\"', 'Ctor', '4', '\":', 'Ġ{', 'ĊĠĠĠĠĠ', 'Ġreturn', 'Ġ_', 'uniq', '_', '1', '1', ';', 'ĊĠĠĠ', 'Ġ}', 'ĊĠĠĠ', 'Ġcase', 'Ġ\"', 'Ctor', '5', '\":', 'Ġ{', 'ĊĠĠĠĠĠ', 'Ġlet', 'Ġ_', 'uniq', '_', '1', '3', 'Ġ=', 'Ġ_', 'uniq', '_', '9', '.', 'f', '1', ';', 'ĊĠĠĠĠĠ', 'Ġlet', 'Ġ_', 'uniq', '_', '1', '2', 'Ġ=', 'Ġ_', 'uniq', '_', '9', '.', 'f', '0', ';', 'ĊĠĠĠĠĠ', 'Ġreturn', 'Ġ_', 'uniq', '_', '6', '(_', 'uniq', '_', '1', '3', ',', 'Ġ_', 'uniq', '_', '1', '0', ',', 'Ġ_', 'uniq', '_', '1', '0', '(_', 'uniq', '_', '1', '1', ',', 'Ġ_', 'uniq', '_', '1', '2', '));', 'ĊĠĠĠ', 'Ġ}', 'ĊĠ', 'Ġ}', 'Ċ', '}', 'Ċ', '<fim_middle>']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "saved_tokens = []\n",
    "def get_all_hidden_states(prompt, max_new_tokens):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            saved_tokens.append(invoker.input.tokens())\n",
    "            print(saved_tokens)\n",
    "            hidden_states = {layer : [] for layer in range(model.config.n_layer)}\n",
    "            for i in range(max_new_tokens-1):\n",
    "                invoker.next()\n",
    "                \n",
    "            for l in range(model.config.n_layer):\n",
    "                hs = model.transformer.h[l].output[0].save()\n",
    "                hidden_states[l].append(model.lm_head(model.transformer.ln_f(hs)).save())\n",
    "    return hidden_states\n",
    "\n",
    "full_hidden_states = get_all_hidden_states(prompt, max_new_tokens)\n",
    "value_hidden_states = {k : [util.apply(i, lambda x: x.value, Proxy) for i in v] for k,v in full_hidden_states.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_0</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>layer_5</th>\n",
       "      <th>layer_6</th>\n",
       "      <th>layer_7</th>\n",
       "      <th>layer_8</th>\n",
       "      <th>layer_9</th>\n",
       "      <th>...</th>\n",
       "      <th>layer_14</th>\n",
       "      <th>layer_15</th>\n",
       "      <th>layer_16</th>\n",
       "      <th>layer_17</th>\n",
       "      <th>layer_18</th>\n",
       "      <th>layer_19</th>\n",
       "      <th>layer_20</th>\n",
       "      <th>layer_21</th>\n",
       "      <th>layer_22</th>\n",
       "      <th>layer_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(ativos, 0.023885412141680717), (áveis, 0.014...</td>\n",
       "      <td>[(eded, 0.04464365169405937), (cheme, 0.040472...</td>\n",
       "      <td>[(gular, 0.18594110012054443), (iteral, 0.0508...</td>\n",
       "      <td>[(gular, 0.26038503646850586), (ayload, 0.0385...</td>\n",
       "      <td>[(ayload, 0.04933283478021622), (gular, 0.0417...</td>\n",
       "      <td>[(ayload, 0.11017901450395584), (chemy, 0.0570...</td>\n",
       "      <td>[(ayload, 0.11401428282260895), (hibited, 0.05...</td>\n",
       "      <td>[(opx, 0.07060158252716064), (ilon, 0.05417048...</td>\n",
       "      <td>[(edata, 0.02658381685614586), (ookie, 0.01994...</td>\n",
       "      <td>[(nown, 0.059704143553972244), (edata, 0.05822...</td>\n",
       "      <td>...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.42255502939224243), (utoff,...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.9999828338623047), (&lt;fim_pa...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.9999886751174927), (&lt;fim_pa...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.999969482421875), (t, 1.307...</td>\n",
       "      <td>[(t, 0.7486408352851868), (&lt;|endoftext|&gt;, 0.10...</td>\n",
       "      <td>[(t, 0.7072942852973938), (q, 0.28672108054161...</td>\n",
       "      <td>[(q, 0.7877132296562195), (t, 0.21189612150192...</td>\n",
       "      <td>[(t, 0.6726053953170776), (q, 0.32711467146873...</td>\n",
       "      <td>[(q, 0.5214332342147827), (t, 0.47795587778091...</td>\n",
       "      <td>[(t, 0.5981452465057373), (q, 0.39379426836967...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             layer_0  \\\n",
       "0  [(ativos, 0.023885412141680717), (áveis, 0.014...   \n",
       "\n",
       "                                             layer_1  \\\n",
       "0  [(eded, 0.04464365169405937), (cheme, 0.040472...   \n",
       "\n",
       "                                             layer_2  \\\n",
       "0  [(gular, 0.18594110012054443), (iteral, 0.0508...   \n",
       "\n",
       "                                             layer_3  \\\n",
       "0  [(gular, 0.26038503646850586), (ayload, 0.0385...   \n",
       "\n",
       "                                             layer_4  \\\n",
       "0  [(ayload, 0.04933283478021622), (gular, 0.0417...   \n",
       "\n",
       "                                             layer_5  \\\n",
       "0  [(ayload, 0.11017901450395584), (chemy, 0.0570...   \n",
       "\n",
       "                                             layer_6  \\\n",
       "0  [(ayload, 0.11401428282260895), (hibited, 0.05...   \n",
       "\n",
       "                                             layer_7  \\\n",
       "0  [(opx, 0.07060158252716064), (ilon, 0.05417048...   \n",
       "\n",
       "                                             layer_8  \\\n",
       "0  [(edata, 0.02658381685614586), (ookie, 0.01994...   \n",
       "\n",
       "                                             layer_9  ...  \\\n",
       "0  [(nown, 0.059704143553972244), (edata, 0.05822...  ...   \n",
       "\n",
       "                                            layer_14  \\\n",
       "0  [(<|endoftext|>, 0.42255502939224243), (utoff,...   \n",
       "\n",
       "                                            layer_15  \\\n",
       "0  [(<|endoftext|>, 0.9999828338623047), (<fim_pa...   \n",
       "\n",
       "                                            layer_16  \\\n",
       "0  [(<|endoftext|>, 0.9999886751174927), (<fim_pa...   \n",
       "\n",
       "                                            layer_17  \\\n",
       "0  [(<|endoftext|>, 0.999969482421875), (t, 1.307...   \n",
       "\n",
       "                                            layer_18  \\\n",
       "0  [(t, 0.7486408352851868), (<|endoftext|>, 0.10...   \n",
       "\n",
       "                                            layer_19  \\\n",
       "0  [(t, 0.7072942852973938), (q, 0.28672108054161...   \n",
       "\n",
       "                                            layer_20  \\\n",
       "0  [(q, 0.7877132296562195), (t, 0.21189612150192...   \n",
       "\n",
       "                                            layer_21  \\\n",
       "0  [(t, 0.6726053953170776), (q, 0.32711467146873...   \n",
       "\n",
       "                                            layer_22  \\\n",
       "0  [(q, 0.5214332342147827), (t, 0.47795587778091...   \n",
       "\n",
       "                                            layer_23  \n",
       "0  [(t, 0.5981452465057373), (q, 0.39379426836967...  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_preds = {layer : [] for layer in range(model.config.n_layer)}\n",
    "# value_hidden_states[14]\n",
    "for layer, hs in value_hidden_states.items():\n",
    "    tok_hs = hs[0]\n",
    "    top_vals_and_idx = tok_hs.softmax(dim=-1).topk(topk)\n",
    "    indices = top_vals_and_idx.indices.tolist()[0][0]\n",
    "    values = top_vals_and_idx.values.tolist()[0][0]\n",
    "    top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "    tok_preds[layer].append(list(zip(top_idx, values)))\n",
    "    \n",
    "tok_preds_df = pd.DataFrame(tok_preds)\n",
    "tok_preds_df.columns = [f\"layer_{i}\" for i in range(model.config.n_layer)]\n",
    "tok_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at logits in prompt and how they change through layers\n",
    "# for each layer, collect the topk predictions for each token in the prompt\n",
    "\n",
    "prompt_tok_preds = {l : [] for l in range(model.config.n_layer)}\n",
    "for layer, hs in value_hidden_states.items():\n",
    "    prompt_hs = hs[0]\n",
    "    for k in range(prompt_hs.shape[1]):\n",
    "        tok_hs = prompt_hs[:, k, :]\n",
    "        top_vals_and_idx = tok_hs.softmax(dim=-1).topk(topk)\n",
    "        indices = top_vals_and_idx.indices[0].tolist()\n",
    "        values = top_vals_and_idx.values.tolist()[0]\n",
    "        top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "        prompt_tok_preds[layer].append(list(zip(top_idx, values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_0</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>layer_5</th>\n",
       "      <th>layer_6</th>\n",
       "      <th>layer_7</th>\n",
       "      <th>layer_8</th>\n",
       "      <th>layer_9</th>\n",
       "      <th>...</th>\n",
       "      <th>layer_14</th>\n",
       "      <th>layer_15</th>\n",
       "      <th>layer_16</th>\n",
       "      <th>layer_17</th>\n",
       "      <th>layer_18</th>\n",
       "      <th>layer_19</th>\n",
       "      <th>layer_20</th>\n",
       "      <th>layer_21</th>\n",
       "      <th>layer_22</th>\n",
       "      <th>layer_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(ativos, 0.023885412141680717), (áveis, 0.014...</td>\n",
       "      <td>[(eded, 0.04464365169405937), (cheme, 0.040472...</td>\n",
       "      <td>[(gular, 0.18594110012054443), (iteral, 0.0508...</td>\n",
       "      <td>[(gular, 0.26038503646850586), (ayload, 0.0385...</td>\n",
       "      <td>[(ayload, 0.04933283478021622), (gular, 0.0417...</td>\n",
       "      <td>[(ayload, 0.11017901450395584), (chemy, 0.0570...</td>\n",
       "      <td>[(ayload, 0.11401428282260895), (hibited, 0.05...</td>\n",
       "      <td>[(opx, 0.07060158252716064), (ilon, 0.05417048...</td>\n",
       "      <td>[(edata, 0.02658381685614586), (ookie, 0.01994...</td>\n",
       "      <td>[(nown, 0.059704143553972244), (edata, 0.05822...</td>\n",
       "      <td>...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.42255502939224243), (utoff,...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.9999828338623047), (&lt;fim_pa...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.9999886751174927), (&lt;fim_pa...</td>\n",
       "      <td>[(&lt;|endoftext|&gt;, 0.999969482421875), (t, 1.307...</td>\n",
       "      <td>[(t, 0.7486408352851868), (&lt;|endoftext|&gt;, 0.10...</td>\n",
       "      <td>[(t, 0.7072942852973938), (q, 0.28672108054161...</td>\n",
       "      <td>[(q, 0.7877132296562195), (t, 0.21189612150192...</td>\n",
       "      <td>[(t, 0.6726053953170776), (q, 0.32711467146873...</td>\n",
       "      <td>[(q, 0.5214332342147827), (t, 0.47795587778091...</td>\n",
       "      <td>[(t, 0.5981452465057373), (q, 0.39379426836967...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             layer_0  \\\n",
       "0  [(ativos, 0.023885412141680717), (áveis, 0.014...   \n",
       "\n",
       "                                             layer_1  \\\n",
       "0  [(eded, 0.04464365169405937), (cheme, 0.040472...   \n",
       "\n",
       "                                             layer_2  \\\n",
       "0  [(gular, 0.18594110012054443), (iteral, 0.0508...   \n",
       "\n",
       "                                             layer_3  \\\n",
       "0  [(gular, 0.26038503646850586), (ayload, 0.0385...   \n",
       "\n",
       "                                             layer_4  \\\n",
       "0  [(ayload, 0.04933283478021622), (gular, 0.0417...   \n",
       "\n",
       "                                             layer_5  \\\n",
       "0  [(ayload, 0.11017901450395584), (chemy, 0.0570...   \n",
       "\n",
       "                                             layer_6  \\\n",
       "0  [(ayload, 0.11401428282260895), (hibited, 0.05...   \n",
       "\n",
       "                                             layer_7  \\\n",
       "0  [(opx, 0.07060158252716064), (ilon, 0.05417048...   \n",
       "\n",
       "                                             layer_8  \\\n",
       "0  [(edata, 0.02658381685614586), (ookie, 0.01994...   \n",
       "\n",
       "                                             layer_9  ...  \\\n",
       "0  [(nown, 0.059704143553972244), (edata, 0.05822...  ...   \n",
       "\n",
       "                                            layer_14  \\\n",
       "0  [(<|endoftext|>, 0.42255502939224243), (utoff,...   \n",
       "\n",
       "                                            layer_15  \\\n",
       "0  [(<|endoftext|>, 0.9999828338623047), (<fim_pa...   \n",
       "\n",
       "                                            layer_16  \\\n",
       "0  [(<|endoftext|>, 0.9999886751174927), (<fim_pa...   \n",
       "\n",
       "                                            layer_17  \\\n",
       "0  [(<|endoftext|>, 0.999969482421875), (t, 1.307...   \n",
       "\n",
       "                                            layer_18  \\\n",
       "0  [(t, 0.7486408352851868), (<|endoftext|>, 0.10...   \n",
       "\n",
       "                                            layer_19  \\\n",
       "0  [(t, 0.7072942852973938), (q, 0.28672108054161...   \n",
       "\n",
       "                                            layer_20  \\\n",
       "0  [(q, 0.7877132296562195), (t, 0.21189612150192...   \n",
       "\n",
       "                                            layer_21  \\\n",
       "0  [(t, 0.6726053953170776), (q, 0.32711467146873...   \n",
       "\n",
       "                                            layer_22  \\\n",
       "0  [(q, 0.5214332342147827), (t, 0.47795587778091...   \n",
       "\n",
       "                                            layer_23  \n",
       "0  [(t, 0.5981452465057373), (q, 0.39379426836967...  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_logits = pd.DataFrame(prompt_tok_preds)\n",
    "prompt_logits.columns = [f\"layer_{i}\" for i in range(model.config.n_layer)]\n",
    "prompt_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_layer = prompt_logits[\"layer_23\"].tolist()\n",
    "# print(len(curr_layer))\n",
    "\n",
    "# probabilities = []\n",
    "# text = []\n",
    "# x = []\n",
    "# minn = 251\n",
    "# for k in range(minn, minn+10):\n",
    "#     token = curr_layer[k]\n",
    "#     token_logit_text = [i[0] for i in token]\n",
    "#     token_logit_prob = [i[1] for i in token]\n",
    "#     probabilities.append(token_logit_prob)\n",
    "#     x.append(saved_tokens[0][k])\n",
    "#     text.append(token_logit_text)\n",
    "    \n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#                     z=probabilities,\n",
    "#                     x = x,\n",
    "#                     text=text,\n",
    "#                     texttemplate=\"%{text}\",\n",
    "#                     textfont={\"size\":15}))\n",
    "\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons why prediction is wrong\n",
    "- 0 is similar to 1 (number issue)\n",
    "    - probably not, can change 0 to q and 1 to t and it still fails\n",
    "- another algorithm picks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Generated:====\n",
      "\n",
      "_uniq_t => _uniq_q,\n",
      "  _uniq_11: _uniq_q\n",
      "): _uniq_q {\n",
      "  switch (_uniq_9.kind) {\n",
      "    case \"Ctor4\": {\n",
      "      return _uniq\n",
      "\n",
      "====Solution:=====\n",
      "\n",
      "_uniq_q\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kr\">declare</span><span class=\"w\"> </span><span class=\"kd\">var</span><span class=\"w\"> </span><span class=\"nx\">require</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">any</span><span class=\"p\">;</span>\n",
       "<span class=\"kd\">const</span><span class=\"w\"> </span><span class=\"nx\">assert</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">require</span><span class=\"p\">(</span><span class=\"s2\">&quot;node:assert&quot;</span><span class=\"p\">);</span>\n",
       "<span class=\"kr\">type</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor2&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor3&quot;</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f0</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"w\"> </span><span class=\"p\">};</span>\n",
       "<span class=\"kr\">type</span><span class=\"w\"> </span><span class=\"nx\">_uniq_t</span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor4&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nx\">kind</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor5&quot;</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f0</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"nx\">f1</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_t</span><span class=\"p\">};</span>\n",
       "\n",
       "<span class=\"kd\">function</span><span class=\"w\"> </span><span class=\"nx\">_uniq_6</span><span class=\"p\">(</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_9</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_t</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_10</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nx\">__x7</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">__x8</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"nx\">FILL</span><span class=\"o\">&gt;</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">  </span><span class=\"nx\">_uniq_11</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">_uniq_q</span>\n",
       "<span class=\"p\">)</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"nx\">_uniq_q</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">  </span><span class=\"k\">switch</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">kind</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"k\">case</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor4&quot;</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">      </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nx\">_uniq_11</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"k\">case</span><span class=\"w\"> </span><span class=\"s2\">&quot;Ctor5&quot;</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">      </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"nx\">_uniq_13</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">f1</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">      </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"nx\">_uniq_12</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nx\">_uniq_9</span><span class=\"p\">.</span><span class=\"nx\">f0</span><span class=\"p\">;</span>\n",
       "<span class=\"w\">      </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nx\">_uniq_6</span><span class=\"p\">(</span><span class=\"nx\">_uniq_13</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_10</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_10</span><span class=\"p\">(</span><span class=\"nx\">_uniq_11</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nx\">_uniq_12</span><span class=\"p\">));</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">  </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kr}{declare}\\PY{+w}{ }\\PY{k+kd}{var}\\PY{+w}{ }\\PY{n+nx}{require}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{any}\\PY{p}{;}\n",
       "\\PY{k+kd}{const}\\PY{+w}{ }\\PY{n+nx}{assert}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{require}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}node:assert\\PYZdq{}}\\PY{p}{)}\\PY{p}{;}\n",
       "\\PY{k+kr}{type}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor2\\PYZdq{}}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{+w}{ }\\PY{o}{|}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor3\\PYZdq{}}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f0}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{p}{;}\n",
       "\\PY{k+kr}{type}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}t}\\PY{o}{=}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor4\\PYZdq{}}\\PY{+w}{ }\\PY{p}{\\PYZcb{}}\\PY{+w}{ }\\PY{o}{|}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\\PY{+w}{ }\\PY{n+nx}{kind}\\PY{o}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor5\\PYZdq{}}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f0}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{;}\\PY{+w}{ }\\PY{n+nx}{f1}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}t}\\PY{p}{\\PYZcb{}}\\PY{p}{;}\n",
       "\n",
       "\\PY{k+kd}{function}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}6}\\PY{p}{(}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}t}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{(}\\PY{n+nx}{\\PYZus{}\\PYZus{}x7}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}\\PYZus{}x8}\\PY{o}{:}\\PY{+w}{ }\\PY{o}{\\PYZlt{}}\\PY{n+nx}{FILL}\\PY{o}{\\PYZgt{}}\\PY{p}{)}\\PY{+w}{ }\\PY{p}{=\\PYZgt{}}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{p}{,}\n",
       "\\PY{+w}{  }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{o}{:}\\PY{+w}{ }\\PY{k+kt}{\\PYZus{}uniq\\PYZus{}q}\n",
       "\\PY{p}{)}\\PY{o}{:}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}q}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{  }\\PY{k}{switch}\\PY{+w}{ }\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{kind}\\PY{p}{)}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{k}{case}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor4\\PYZdq{}}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{      }\\PY{k}{return}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{p}{;}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{k}{case}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}Ctor5\\PYZdq{}}\\PY{o}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{      }\\PY{k+kd}{let}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}13}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{f1}\\PY{p}{;}\n",
       "\\PY{+w}{      }\\PY{k+kd}{let}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}12}\\PY{+w}{ }\\PY{o}{=}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}9}\\PY{p}{.}\\PY{n+nx}{f0}\\PY{p}{;}\n",
       "\\PY{+w}{      }\\PY{k}{return}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}6}\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}13}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}10}\\PY{p}{(}\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}11}\\PY{p}{,}\\PY{+w}{ }\\PY{n+nx}{\\PYZus{}uniq\\PYZus{}12}\\PY{p}{)}\\PY{p}{)}\\PY{p}{;}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{  }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "declare var require: any;\n",
       "const assert = require(\"node:assert\");\n",
       "type _uniq_q = { kind: \"Ctor2\" } | { kind: \"Ctor3\"; f0: _uniq_q };\n",
       "type _uniq_t= { kind: \"Ctor4\" } | { kind: \"Ctor5\"; f0: _uniq_q; f1: _uniq_t};\n",
       "\n",
       "function _uniq_6(\n",
       "  _uniq_9: _uniq_t\n",
       "  _uniq_10: (__x7: _uniq_q, __x8: <FILL>) => _uniq_q,\n",
       "  _uniq_11: _uniq_q\n",
       "): _uniq_q {\n",
       "  switch (_uniq_9.kind) {\n",
       "    case \"Ctor4\": {\n",
       "      return _uniq_11;\n",
       "    }\n",
       "    case \"Ctor5\": {\n",
       "      let _uniq_13 = _uniq_9.f1;\n",
       "      let _uniq_12 = _uniq_9.f0;\n",
       "      return _uniq_6(_uniq_13, _uniq_10, _uniq_10(_uniq_11, _uniq_12));\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\"====Generated:====\\n\\n{generated}\\n\\n====Solution:=====\\n\\n{solution}\")\n",
    "code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attn visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "GPTBigCodeModel is using GPTBigCodeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` and `head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 101376.0\n"
     ]
    }
   ],
   "source": [
    "# TODO something wrong here, always gives same resd\n",
    "attn, toks = attention_vis(model, prompt+\"_uniq_\")\n",
    "print(len(attn), torch.stack(attn).sum().item())\n",
    "# b, h, q, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 264, 264])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [9.9998385e-01, 5.6785061e-03, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [9.9999410e-01, 1.9448635e-03, 2.8284541e-03, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        ...,\n",
       "        [8.2785435e-02, 1.6548799e-04, 9.0345355e-05, ..., 2.8032113e-03,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [8.4791221e-02, 6.9219786e-05, 3.4148121e-05, ..., 2.1113926e-03,\n",
       "         2.6780353e-03, 0.0000000e+00],\n",
       "        [1.2193324e-01, 6.7537971e-05, 3.2085631e-05, ..., 7.3519088e-03,\n",
       "         1.7220268e-03, 5.8213212e-03]], dtype=float32),\n",
       " tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [9.9435e-01, 5.6465e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [9.9525e-01, 1.9356e-03, 2.8150e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [2.4430e-02, 4.8835e-05, 2.6660e-05,  ..., 8.2721e-04, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.6812e-02, 2.1888e-05, 1.0798e-05,  ..., 6.6764e-04, 8.4682e-04,\n",
       "          0.0000e+00],\n",
       "         [4.5582e-02, 2.5248e-05, 1.1995e-05,  ..., 2.7484e-03, 6.4374e-04,\n",
       "          2.1762e-03]], device='cuda:3'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def normalize_attn(attention_matrix):\n",
    "    return normalize(attention_matrix, norm=\"l2\", axis=1)\n",
    "\n",
    "print(attn[0].shape)\n",
    "normalize_attn(attn[13][3].cpu().detach().numpy()), attn[13][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cv.attention.attention_pattern(tokens=no_special_tok[window_idx:], attention=attn[14][window_idx:], mask_upper_tri=True)\n",
    "# query is destination, key is source\n",
    "\n",
    "# heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "def create_attention_heatmap(attention_matrix, tokens, row_idx, column_idx, step=10):\n",
    "    normalized_attention = normalize_attn(attention_matrix)\n",
    "    \n",
    "    # Get a subset of the attention matrix\n",
    "    subset_matrix = normalized_attention[row_idx:row_idx+step, column_idx:column_idx+step]\n",
    "    xtokens = tokens[column_idx:column_idx+step]\n",
    "    ytokens = tokens[row_idx:row_idx+step]\n",
    "\n",
    "    \n",
    "    # Create a heatmap using Seaborn\n",
    "    sns.set()\n",
    "\n",
    "    sns.heatmap(subset_matrix, cmap=\"viridis\", xticklabels=xtokens, yticklabels=ytokens)\n",
    "    # annot=subset_matrix, fmt=\".2f\", annot_kws={\"size\": 10}\n",
    "    \n",
    "    plt.xlabel(\"Quert DST Tokens\")\n",
    "    plt.ylabel(\"Key SRC Tokens\")\n",
    "\n",
    "\n",
    "\n",
    "layers = []\n",
    "heads = list(range(16))\n",
    "window_range = list(range(0, len(toks)-10, 10))\n",
    "\n",
    "column_idx = len(toks)-10\n",
    "step = 10\n",
    "full_sum = 0\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "    \n",
    "for layer in tqdm(layers):\n",
    "    os.makedirs(f\"images/layer_{layer}\", exist_ok=True)\n",
    "\n",
    "    for row_idx in window_range:\n",
    "        xtokens = {i:t for i,t in enumerate(toks[column_idx:column_idx+step])}\n",
    "        ytokens = {i:t for i,t in enumerate(toks[row_idx:row_idx+step])}\n",
    "        # print(\"X DST:\\n\",xtokens, \"\\nY SRC:\\n\", ytokens)\n",
    "        for h in heads:\n",
    "            # create a subplot for this head    \n",
    "            plt.subplot(4,4, h+1)\n",
    "            kq = attn[layer][h].cpu().detach().numpy()\n",
    "            \n",
    "            summ = kq[row_idx:row_idx+step, column_idx:column_idx+step].sum().item()\n",
    "            if summ == 0:\n",
    "                continue\n",
    "            full_sum += summ\n",
    "            create_attention_heatmap(kq, toks, row_idx=row_idx, column_idx=column_idx, step=step)\n",
    "        \n",
    "        if full_sum > 0:\n",
    "            # print(f\"Saving {h} head at row {row_idx}\")\n",
    "            full_sum = 0\n",
    "            # save\n",
    "            plt.tight_layout()\n",
    "            with open(f\"images/layer_{layer}/row_{row_idx}_column_{column_idx}_step_{step}.txt\", \"w\") as f:\n",
    "                f.write(f\"X DST:\\n{xtokens}\\nY SRC:\\n{ytokens}\")\n",
    "            plt.savefig(f\"images/layer_{layer}/row_{row_idx}_column_{column_idx}_step_{step}.png\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer erase - logit difference\n",
    "\n",
    "erasing doesn't quite work -> maybe try noise or random patch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Accessing Proxy value before it's been set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m hidden_states_erase \u001b[38;5;241m=\u001b[39m get_final_states_erase(prompt, \u001b[38;5;241m4\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m24\u001b[39m)))\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m [util\u001b[38;5;241m.\u001b[39mapply(i, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mvalue, Proxy) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states]\n\u001b[0;32m---> 29\u001b[0m out_erase \u001b[38;5;241m=\u001b[39m [util\u001b[38;5;241m.\u001b[39mapply(i, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mvalue, Proxy) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states_erase]\n\u001b[1;32m     31\u001b[0m logit_diffs_q \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m logit_diffs_t \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m hidden_states_erase \u001b[38;5;241m=\u001b[39m get_final_states_erase(prompt, \u001b[38;5;241m4\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m24\u001b[39m)))\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m [util\u001b[38;5;241m.\u001b[39mapply(i, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mvalue, Proxy) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states]\n\u001b[0;32m---> 29\u001b[0m out_erase \u001b[38;5;241m=\u001b[39m [\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProxy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states_erase]\n\u001b[1;32m     31\u001b[0m logit_diffs_q \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m logit_diffs_t \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/venvs/gpu/lib/python3.10/site-packages/nnsight/util.py:22\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(data, fn, cls)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies some function to all members of a collection of a give type (or types)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Collection: Same kind of collection as data, after then fn has been applied to members of given type.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [apply(_data, fn, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _data \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m hidden_states_erase \u001b[38;5;241m=\u001b[39m get_final_states_erase(prompt, \u001b[38;5;241m4\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m24\u001b[39m)))\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m [util\u001b[38;5;241m.\u001b[39mapply(i, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mvalue, Proxy) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states]\n\u001b[0;32m---> 29\u001b[0m out_erase \u001b[38;5;241m=\u001b[39m [util\u001b[38;5;241m.\u001b[39mapply(i, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m, Proxy) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hidden_states_erase]\n\u001b[1;32m     31\u001b[0m logit_diffs_q \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m logit_diffs_t \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/venvs/gpu/lib/python3.10/site-packages/nnsight/intervention.py:162\u001b[0m, in \u001b[0;36mInterventionProxy.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Property to return the value of this proxy's node.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Any: The stored value of the proxy, populated during execution of the model.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing Proxy value before it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms been set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mValueError\u001b[0m: Accessing Proxy value before it's been set."
     ]
    }
   ],
   "source": [
    "def get_final_states_erase(prompt, max_new_tokens, layers = []):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            hidden_states = []\n",
    "                    \n",
    "            for i in range(max_new_tokens):\n",
    "\n",
    "                for l in range(model.config.n_layer):\n",
    "                    if l in layers:\n",
    "                        if l == 0:\n",
    "                            raise ValueError(\"Cannot erase layer 0\")\n",
    "                        else:\n",
    "                            model.transformer.h[l].output = last_saved\n",
    "                    else:\n",
    "                        last_saved = model.transformer.h[l].output.save()\n",
    "                final_hs = model.lm_head.output\n",
    "                hidden_states.append(final_hs.save())\n",
    "                invoker.next()\n",
    "             \n",
    "            hidden_states.append(final_hs.save())\n",
    "            \n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "hidden_states = get_final_states(prompt, 4)\n",
    "hidden_states_erase = get_final_states_erase(prompt, 4, layers=list(range(2,24)))\n",
    "\n",
    "out = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states]\n",
    "out_erase = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states_erase]\n",
    "\n",
    "logit_diffs_q = []\n",
    "logit_diffs_t = []\n",
    "\n",
    "q_idx = model.tokenizer.encode(\"q\")[0]\n",
    "t_idx = model.tokenizer.encode(\"t\")[0]\n",
    "\n",
    "topk=10\n",
    "predictions = []\n",
    "for j in range(len(out)):\n",
    "    hs = out[j]\n",
    "    hs_erase = out_erase[j]\n",
    "    if j == 0:\n",
    "        hs = hs[:, -1,:]\n",
    "        hs = hs.unsqueeze(1)\n",
    "        hs_erase = hs_erase[:, -1,:]\n",
    "        hs_erase = hs_erase.unsqueeze(1)\n",
    "        \n",
    "    logits_clean = hs.softmax(dim=-1)\n",
    "    logits_erase = hs_erase.softmax(dim=-1)\n",
    "    \n",
    "    logit_clean_q = logits_clean[:,:,q_idx].item()\n",
    "    logit_erase_q = logits_erase[:,:,q_idx].item()\n",
    "    logit_clean_t = logits_clean[:,:,t_idx].item()\n",
    "    logit_erase_t = logits_erase[:,:,t_idx].item()\n",
    "    \n",
    "    max_logit_clean = logits_clean.argmax().item()\n",
    "    max_logit_erase = logits_erase.argmax().item()\n",
    "    \n",
    "    logit_diffs_q.append([logit_clean_q - logit_erase_q, logit_clean_q, logit_erase_q])\n",
    "    logit_diffs_t.append([logit_clean_t - logit_erase_t, logit_clean_t, logit_erase_t])\n",
    "    \n",
    "erase_prediction = model.tokenizer.decode(max_logit_erase)\n",
    "clean_prediction = model.tokenizer.decode(max_logit_clean)\n",
    "print(f\"Clean: {clean_prediction}, Erase: {erase_prediction}\")\n",
    "\n",
    "result_table = pd.DataFrame([logit_diffs_q[-1], logit_diffs_t[-1]], index=[\"q\",\"t\"], columns=[\"diff\", \"clean\", \"erase\"])\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
