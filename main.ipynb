{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNsight Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# # # set cuda visible device\n",
    "# !export CUDA_VISIBLE_DEVICES=3\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "starcoderbase_1b = \"/home/arjun/models/starcoderbase-1b/\"\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(starcoderbase_1b, device_map='cuda:3')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "\n",
    "import datasets\n",
    "from src.utils import *\n",
    "\n",
    "ds = datasets.load_dataset(\"franlucc/type_patching_v0\", split=\"train\")\n",
    "string_ex = [d for d in ds if d[\"fim_type\"] == \"string\"]\n",
    "boolean_ex = [d for d in ds if d[\"fim_type\"] == \"boolean\"]\n",
    "number_ex = [d for d in ds if d[\"fim_type\"] == \"number\"]\n",
    "\n",
    "string_idx = model.tokenizer.convert_tokens_to_ids(\"string\")\n",
    "boolean_idx = model.tokenizer.convert_tokens_to_ids(\"boolean\")\n",
    "number_idx = model.tokenizer.convert_tokens_to_ids(\"number\")\n",
    "print(string_idx, boolean_idx, number_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.type_patching import *\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "pred_results = []\n",
    "probs_patched_results = []\n",
    "earliest_layer = []\n",
    "def get_ds(patch):\n",
    "    if patch == \"string\":\n",
    "        return string_ex\n",
    "    elif patch == \"boolean\":\n",
    "        return boolean_ex\n",
    "    elif patch == \"number\":\n",
    "        return number_ex\n",
    "\n",
    "patch_src = \"string\"\n",
    "patch_dst = \"boolean\"\n",
    "correct_index = model.tokenizer(patch_src)[\"input_ids\"][0]\n",
    "incorrect_index = model.tokenizer(patch_dst)[\"input_ids\"][0]\n",
    "\n",
    "idx_range = (80,90)\n",
    "\n",
    "for i in range(*idx_range):\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    clean_prompt = placeholder_to_std_fmt(get_ds(patch_src)[i][\"fim_program\"], STARCODER_FIM)\n",
    "    corrupted_prompt = placeholder_to_std_fmt(get_ds(patch_dst)[i][\"fim_program\"], STARCODER_FIM)\n",
    "    patching_results, patched_predictions = patch_fim_tokens(model, clean_prompt, corrupted_prompt, STARCODER_FIM, correct_index, incorrect_index)\n",
    "    \n",
    "    patched_predictions = util.apply(patched_predictions, lambda x: x.value.item(), Proxy)\n",
    "    patching_results = util.apply(patching_results, lambda x: x.value.item(), Proxy)\n",
    "    \n",
    "    probs_patched_results.append(patching_results)\n",
    "    pred_results.append(patched_predictions)\n",
    "    \n",
    "    idx = [j for j, x in enumerate(patched_predictions) if x == correct_index]\n",
    "    if len(idx) == 0:\n",
    "        idx = [-1]\n",
    "    earliest_layer.append(idx[0])\n",
    "\n",
    "    # clear gpu memory\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(range(8,15))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# import numpy as np\n",
    "# colours = im.cmap(im.norm(np.unique(pred_results)))\n",
    "# len_colors = len(colours)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "# make labels\n",
    "flat_results = [x for y in pred_results for x in y]\n",
    "labels = set([model.tokenizer.decode([x]) for x in flat_results])\n",
    "labels = list(set(flat_results))\n",
    "labels.sort()\n",
    "print(labels)\n",
    "\n",
    "\n",
    "# set color map\n",
    "im = plt.imshow(pred_results, norm=\"log\", cmap=\"hsv\")\n",
    "\n",
    "plt.yticks(range(len(pred_results)), [f\"Example {i}\" for i in range(len(pred_results))])\n",
    "plt.xticks(range(len(layers)), layers)\n",
    "plt.xlabel(\"Patched Layer\")\n",
    "plt.ylabel(\"Src->Dst Example idx\")\n",
    "plt.title(f\"Starcoderbase-1b. Max probability token after patching <fim_middle> from {patch_dst} to {patch_src}\")\n",
    "\n",
    "# make legend\n",
    "\n",
    "# get map of labels to colors from image\n",
    "color_map = im.cmap(im.norm(np.unique(pred_results)))\n",
    "# use map to make legend\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1,0.5), handles=[plt.Rectangle((0,0),1,1, color=color_map[i], label=model.tokenizer.decode(labels[i])) for i in range(len(labels))])\n",
    "\n",
    "\n",
    "# add grid between examples and layers (halfway!)\n",
    "plt.hlines([i+0.5 for i in range(len(pred_results))], *plt.xlim(), color=\"black\", linewidth=1)\n",
    "plt.vlines([i+0.5 for i in range(len(layers))], *plt.ylim(), color=\"black\", linewidth=1)\n",
    "\n",
    "\n",
    "# build an annotations dict for each square in grid with values from probs_patched_results\n",
    "annotations = {i:{} for i in range(len(layers)*len(pred_results))}\n",
    "\n",
    "for i in range(len(pred_results)):\n",
    "    for j in range(len(layers)):\n",
    "        probs = probs_patched_results[i][j]\n",
    "        str_probs = probs[0]\n",
    "        num_probs = probs[1]\n",
    "        bool_probs = probs[2]\n",
    "        annotations[i+len(pred_results)*j] = {\"pred\": pred_results[i][j],\"string\": f\"{str_probs:.2f}\", \"number\": f\"{num_probs:.2f}\", \"boolean\": f\"{bool_probs:.2f}\"}\n",
    "        \n",
    "# create tuples of positions\n",
    "positions =[(x , y ) for x in range(len(layers)) for y in range(len(pred_results))]\n",
    "\n",
    "# add annotations\n",
    "for pos, text in annotations.items():\n",
    "    plt.annotate(model.tokenizer.decode(text[\"pred\"]), xy=positions[pos],color=\"black\", fontsize=8, ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "plt.savefig(f\"starcoderbase-1b-patching_idx{idx_range[0]}.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# # add annotations\n",
    "# for pos, text in annotations.items():\n",
    "#     plt.annotate(text[\"string\"], xy=positions[pos], ha=\"left\", va=\"bottom\", color=\"black\")\n",
    "#     plt.annotate(text[\"number\"], xy=positions[pos], ha=\"center\", va=\"center\", color=\"black\")\n",
    "#     plt.annotate(text[\"boolean\"], xy=positions[pos], ha=\"right\", va=\"top\", color=\"black\") \n",
    "\n",
    "# plt.savefig(f\"starcoderbase-1b-patching_probs_idx{idx_range[0]}.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "annotations, pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
