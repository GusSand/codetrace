392
278
Training...
Batch loss: 138.25794982910156
Batch loss: 16.117918014526367
Batch loss: 8.188323020935059
Batch loss: 9.047972679138184
Batch loss: 7.2576189041137695
Batch loss: 12.98059368133545
Batch loss: 2.1985228061676025
Batch loss: 37.126731872558594
Batch loss: 22.719236373901367
Batch loss: 165.3380584716797
Batch loss: 3.576230764389038
Batch loss: 18.839902877807617
Batch loss: 107.01537322998047
Batch loss: 37.78252029418945
Batch loss: 1.139569878578186
Batch loss: 12.387373924255371
Batch loss: 56.410621643066406
Batch loss: 22.410490036010742
Batch loss: 20.432931900024414
Batch loss: 95.38450622558594
Batch loss: 3.3072738647460938
Batch loss: 28.04669189453125
Batch loss: 16.28010368347168
Batch loss: -1.3327910900115967
Batch loss: 5.46038818359375
Batch loss: 15.715097427368164
Batch loss: 6.764695644378662
Batch loss: 93.59356689453125
Batch loss: 41.034542083740234
Batch loss: 47.40998077392578
Batch loss: 0.10123863071203232
Batch loss: 3.02250075340271
Batch loss: 15.506512641906738
Batch loss: 5.540432929992676
Batch loss: 43.10231399536133
Batch loss: 28.343915939331055
Batch loss: 46.81797409057617
Batch loss: 20.309391021728516
Batch loss: 29.478729248046875
Batch loss: 7.748008728027344
Batch loss: 30.802261352539062
Batch loss: 136.81248474121094
Batch loss: 51.116241455078125
Batch loss: 91.93343353271484
Batch loss: 25.883193969726562
Batch loss: 70.00846099853516
Batch loss: 47.72907257080078
Batch loss: 28.506372451782227
Batch loss: 435.88897705078125
Batch loss: 28.401464462280273
Batch loss: 30.452661514282227
Batch loss: 168.20567321777344
Batch loss: 268.4947814941406
Batch loss: 72.2094955444336
Batch loss: 7.8270769119262695
Batch loss: 8.294681549072266
Epoch: 1 	Training Loss: 49.168381
Batch loss: 138.25794982910156
Batch loss: 16.117918014526367
Batch loss: 8.188323020935059
Batch loss: 9.047972679138184
Batch loss: 7.2576189041137695
Batch loss: 12.98059368133545
Batch loss: 2.1985228061676025
Batch loss: 37.126731872558594
Batch loss: 22.719236373901367
Batch loss: 165.3380584716797
Batch loss: 3.576230764389038
Batch loss: 18.839902877807617
Batch loss: 107.01537322998047
Batch loss: 37.78252029418945
Batch loss: 1.139569878578186
Batch loss: 12.387373924255371
Batch loss: 56.410621643066406
Batch loss: 22.410490036010742
Batch loss: 20.432931900024414
Batch loss: 95.38450622558594
Batch loss: 3.3072738647460938
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 283, in <module>
    train_loop(llm)
  File "/home/franlucc/projects/codetrace/train.py", line 204, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 80, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 69, in __exit__
    raise exc_val
  File "/home/franlucc/projects/codetrace/train.py", line 85, in apply_mask
    with generator.invoke(prompts) as invoker:
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Invoker.py", line 62, in __enter__
    self.tracer.model._scan(self.input, *self.tracer.args, **self.tracer.kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 386, in _scan
    return self.meta_model(**prepared_inputs.copy().to("meta"))
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/module.py", line 100, in __call__
    return super().__call__(*args, **kwds)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 1265, in forward
    transformer_outputs = self.transformer(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/module.py", line 100, in __call__
    return super().__call__(*args, **kwds)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 1118, in forward
    outputs = block(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/module.py", line 100, in __call__
    return super().__call__(*args, **kwds)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 763, in forward
    hidden_states = self.ln_2(hidden_states)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/module.py", line 100, in __call__
    return super().__call__(*args, **kwds)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 196, in forward
    return F.layer_norm(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/functional.py", line 2540, in layer_norm
    return handle_torch_function(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/overrides.py", line 1560, in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/utils/_device.py", line 77, in __torch_function__
    return func(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/functional.py", line 2543, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/_refs/__init__.py", line 3111, in native_layer_norm
    out, mean, rstd = _normalize(input, reduction_dims, eps)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/_refs/__init__.py", line 2987, in _normalize
    computation_dtype = utils.get_computation_dtype(a.dtype)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/_prims_common/__init__.py", line 1168, in get_computation_dtype
    return _computation_dtype_map.get(dtype, dtype)
KeyboardInterrupt