392
278
Training...
Batch loss: 2.0683534145355225
Batch loss: 0.6465809941291809
Batch loss: 1.3376915454864502
Batch loss: -0.02612254023551941
Batch loss: -0.12248481810092926
Batch loss: -0.5266117453575134
Batch loss: -0.3672214150428772
Batch loss: -0.35077545046806335
Batch loss: -0.2650572955608368
Batch loss: 1.44303297996521
Batch loss: 0.2732718884944916
Batch loss: 0.7236719131469727
Batch loss: 0.34530171751976013
Batch loss: -0.3710717260837555
Batch loss: 1.4175657033920288
Batch loss: -0.16107606887817383
Batch loss: 0.1148931011557579
Batch loss: -0.6962757706642151
Batch loss: -0.31317034363746643
Batch loss: 0.13221736252307892
Batch loss: -0.27658048272132874
Batch loss: 1.5712436437606812
Batch loss: 0.16135139763355255
Batch loss: 5.563268184661865
Batch loss: 0.34685254096984863
Batch loss: 0.7454933524131775
Batch loss: -0.13851268589496613
Batch loss: -0.6014899015426636
Batch loss: 0.1869705766439438
Batch loss: 1.0921499729156494
Batch loss: -0.33080849051475525
Batch loss: 0.9799699783325195
Batch loss: 0.8236047625541687
Batch loss: 0.28628891706466675
Batch loss: 0.3755856156349182
Batch loss: 1.8223856687545776
Batch loss: 0.1820293515920639
Batch loss: 0.1656133532524109
Batch loss: -0.7554911971092224
Batch loss: 1.2001837491989136
Batch loss: 0.7965925335884094
Batch loss: 0.9913080334663391
Batch loss: 3.959503173828125
Batch loss: 0.27154964208602905
Batch loss: 0.04139721393585205
Batch loss: 1.7958115339279175
Batch loss: 0.2821688652038574
Batch loss: -0.3630857467651367
Batch loss: -0.036440301686525345
Batch loss: 0.2592029571533203
Batch loss: 0.6368317008018494
Batch loss: 2.4935038089752197
Batch loss: 0.5003575682640076
Batch loss: 0.43815407156944275
Batch loss: 0.24142085015773773
Batch loss: -0.03933626413345337
Epoch: 1 	Training Loss: 0.553067
Batch loss: 2.0683534145355225
Batch loss: 0.6465809941291809
Batch loss: 1.3376915454864502
Batch loss: -0.02612254023551941
Batch loss: -0.12248481810092926
Batch loss: -0.5266117453575134
Batch loss: -0.3672214150428772
Batch loss: -0.35077545046806335
Batch loss: -0.2650572955608368
Batch loss: 1.44303297996521
Batch loss: 0.2732718884944916
Batch loss: 0.7236719131469727
Batch loss: 0.34530171751976013
Batch loss: -0.3710717260837555
Batch loss: 1.4175657033920288
Batch loss: -0.16107606887817383
Batch loss: 0.1148931011557579
Batch loss: -0.6962757706642151
Batch loss: -0.31317034363746643
Batch loss: 0.13221736252307892
Batch loss: -0.27658048272132874
Batch loss: 1.5712436437606812
Batch loss: 0.16135139763355255
Batch loss: 5.563268184661865
Batch loss: 0.34685254096984863
Batch loss: 0.7454933524131775
Batch loss: -0.13851268589496613
Batch loss: -0.6014899015426636
Batch loss: 0.1869705766439438
Batch loss: 1.0921499729156494
Batch loss: -0.33080849051475525
Batch loss: 0.9799699783325195
Batch loss: 0.8236047625541687
Batch loss: 0.28628891706466675
Batch loss: 0.3755856156349182
Batch loss: 1.8223856687545776
Batch loss: 0.1820293515920639
Batch loss: 0.1656133532524109
Batch loss: -0.7554911971092224
Batch loss: 1.2001837491989136
Batch loss: 0.7965925335884094
Batch loss: 0.9913080334663391
Batch loss: 3.959503173828125
Batch loss: 0.27154964208602905
Batch loss: 0.04139721393585205
Batch loss: 1.7958115339279175
Batch loss: 0.2821688652038574
Batch loss: -0.3630857467651367
Batch loss: -0.036440301686525345
Batch loss: 0.2592029571533203
Batch loss: 0.6368317008018494
Batch loss: 2.4935038089752197
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 283, in <module>
    train_loop(llm)
  File "/home/franlucc/projects/codetrace/train.py", line 204, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 80, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 73, in __exit__
    self.run_local()
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 78, in run_local
    self.output = self.model(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 194, in __call__
    output = fn(inputs, *args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/LanguageModel.py", line 166, in _generation
    return super()._generation(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 412, in _generation
    return self.local_model.generate(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 2392, in greedy_search
    if unfinished_sequences.max() == 0:
KeyboardInterrupt