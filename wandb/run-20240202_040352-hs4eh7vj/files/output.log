264
216
Training...
Batch loss: 0.4707256257534027
Batch loss: 0.3679182231426239
Batch loss: 0.32634010910987854
Batch loss: 0.47015976905822754
Batch loss: 0.21958374977111816
Batch loss: 0.45560646057128906
Batch loss: 0.4969281256198883
Batch loss: 0.5182725787162781
Batch loss: 0.37382420897483826
Batch loss: 0.44991493225097656
Batch loss: 0.4366484582424164
Batch loss: 0.19257937371730804
Batch loss: 0.6300875544548035
Batch loss: 0.7446489930152893
Batch loss: 0.5025257468223572
Batch loss: 0.4958389401435852
Batch loss: 0.3515441119670868
Batch loss: 0.2889450788497925
Batch loss: 0.3402414917945862
Batch loss: 0.583751380443573
Batch loss: 0.5661031603813171
Batch loss: 0.4406174123287201
Batch loss: 0.4859789311885834
Batch loss: 0.31647977232933044
Batch loss: 0.5390822291374207
Batch loss: 0.4723729193210602
Batch loss: 0.5885632634162903
Batch loss: 0.688285231590271
Batch loss: 0.3432140350341797
Batch loss: 0.34305405616760254
Batch loss: 0.39903807640075684
Batch loss: 0.3442230820655823
Batch loss: 0.4224804937839508
Batch loss: 0.463837206363678
Batch loss: 0.44902125000953674
Batch loss: 0.42238903045654297
Batch loss: 0.3621044158935547
Batch loss: 0.4463520646095276
Batch loss: 0.6351419687271118
Batch loss: 0.3921080231666565
Batch loss: 0.5751482248306274
Batch loss: 0.6246954202651978
Batch loss: 0.33111366629600525
Epoch: 1 	Training Loss: 0.440170
Batch loss: 0.5195061564445496
Batch loss: 0.3793522119522095
Batch loss: 0.32830873131752014
Batch loss: 0.4985221028327942
Batch loss: 0.3091006875038147
Batch loss: 0.43339458107948303
Batch loss: 0.5457993745803833
Batch loss: 0.49370917677879333
Batch loss: 0.41928228735923767
Batch loss: 0.44619303941726685
Batch loss: 0.46332645416259766
Batch loss: 0.2617574632167816
Batch loss: 0.5807281732559204
Batch loss: 0.7593107223510742
Batch loss: 0.561558187007904
Batch loss: 0.5244256854057312
Batch loss: 0.2570172846317291
Batch loss: 0.2829844057559967
Batch loss: 0.3087323307991028
Batch loss: 0.5738284587860107
Batch loss: 0.5986929535865784
Batch loss: 0.4849555194377899
Batch loss: 0.5037643313407898
Batch loss: 0.31087252497673035
Batch loss: 0.5156943202018738
Batch loss: 0.4788148105144501
Batch loss: 0.605128288269043
Batch loss: 0.6840258836746216
Batch loss: 0.36740919947624207
Batch loss: 0.32165321707725525
Batch loss: 0.4013690650463104
Batch loss: 0.383800208568573
Batch loss: 0.38311105966567993
Batch loss: 0.46228066086769104
Batch loss: 0.4814889430999756
Batch loss: 0.4637046754360199
Batch loss: 0.3665403723716736
Batch loss: 0.4062363803386688
Batch loss: 0.6501049399375916
Batch loss: 0.4505259096622467
Batch loss: 0.5693406462669373
Batch loss: 0.6628634929656982
Batch loss: 0.32276245951652527
Epoch: 2 	Training Loss: 0.450499
Batch loss: 0.4580349028110504
Batch loss: 0.3926370143890381
Batch loss: 0.336157888174057
Batch loss: 0.49936142563819885
Batch loss: 0.2818775773048401
Batch loss: 0.48092126846313477
Batch loss: 0.516312301158905
Batch loss: 0.4640459716320038
Batch loss: 0.4073837697505951
Batch loss: 0.4409560263156891
Batch loss: 0.44714537262916565
Batch loss: 0.24746525287628174
Batch loss: 0.6212021708488464
Batch loss: 0.7470415234565735
Batch loss: 0.5077440142631531
Batch loss: 0.5366536974906921
Batch loss: 0.3083663582801819
Batch loss: 0.3034014105796814
Batch loss: 0.3590662181377411
Batch loss: 0.5907277464866638
Batch loss: 0.6347200274467468
Batch loss: 0.4003464877605438
Batch loss: 0.507336437702179
Batch loss: 0.27712282538414
Batch loss: 0.5804465413093567
Batch loss: 0.49099746346473694
Batch loss: 0.6150851249694824
Batch loss: 0.6895992755889893
Batch loss: 0.36847397685050964
Batch loss: 0.31916138529777527
Batch loss: 0.4397404193878174
Batch loss: 0.34043845534324646
Batch loss: 0.3859504759311676
Batch loss: 0.42866250872612
Batch loss: 0.46080532670021057
Batch loss: 0.43281683325767517
Batch loss: 0.39378347992897034
Batch loss: 0.3998517096042633
Batch loss: 0.5992159247398376
Batch loss: 0.3231828510761261
Batch loss: 0.5576383471488953
Batch loss: 0.6222427487373352
Batch loss: 0.31467366218566895
Epoch: 3 	Training Loss: 0.443836
Batch loss: 0.47607022523880005
Batch loss: 0.33375516533851624
Batch loss: 0.32654452323913574
Batch loss: 0.4890406131744385
Batch loss: 0.2412717342376709
Batch loss: 0.4989219307899475
Batch loss: 0.5729450583457947
Batch loss: 0.4991318881511688
Batch loss: 0.4036961495876312
Batch loss: 0.41584500670433044
Batch loss: 0.45610639452934265
Batch loss: 0.19582277536392212
Batch loss: 0.6311545372009277
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 276, in <module>
  File "/home/franlucc/projects/codetrace/train.py", line 196, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 79, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 73, in __exit__
    self.run_local()
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 78, in run_local
    self.output = self.model(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 200, in __call__
    gc.collect()
KeyboardInterrupt