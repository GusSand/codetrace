392
278
Training...
Batch loss: 0.761509358882904
Batch loss: 6.82588529586792
Batch loss: 40.179622650146484
Batch loss: 0.2891926169395447
Batch loss: 0.5028294324874878
Batch loss: 1.1457736492156982
Batch loss: 3.3170430660247803
Batch loss: 3.7823474407196045
Batch loss: 0.9270302653312683
Batch loss: 2.300811767578125
Batch loss: 74.18414306640625
Batch loss: 18.081932067871094
Batch loss: 1.3795506954193115
Batch loss: 3.0181379318237305
Batch loss: 0.997663676738739
Batch loss: 5.6391520500183105
Batch loss: 25.329444885253906
Batch loss: 3.470024585723877
Batch loss: 3.978656768798828
Batch loss: 6.673076629638672
Batch loss: 2.347717761993408
Batch loss: 13.276265144348145
Batch loss: 2.411156415939331
Batch loss: 0.7203836441040039
Batch loss: 8.613226890563965
Batch loss: 2.088056802749634
Batch loss: 3.507716417312622
Batch loss: 0.7508487105369568
Batch loss: 0.9328458905220032
Batch loss: -0.6192418932914734
Batch loss: 2.138033628463745
Batch loss: 1.2310659885406494
Batch loss: -0.293487012386322
Batch loss: 1.87039315700531
Batch loss: 0.12293899059295654
Batch loss: 0.794084906578064
Batch loss: 1.4923007488250732
Batch loss: 1.2923544645309448
Batch loss: 7.978628635406494
Batch loss: 0.40776005387306213
Batch loss: 1.4844790697097778
Batch loss: 1.5922073125839233
Batch loss: 0.6069231033325195
Batch loss: 2.0644350051879883
Batch loss: 7.973460674285889
Batch loss: 1.3271105289459229
Batch loss: 1.0178886651992798
Batch loss: 16.702123641967773
Batch loss: 1.351946473121643
Batch loss: 3.0588364601135254
Batch loss: 1.301074504852295
Batch loss: 0.38594746589660645
Batch loss: 1.8136978149414062
Batch loss: 0.27217647433280945
Batch loss: 1.0872300863265991
Batch loss: 3.426307439804077
Epoch: 1 	Training Loss: 5.344870
Batch loss: 0.761509358882904
Batch loss: 6.82588529586792
Batch loss: 40.179622650146484
Batch loss: 0.2891926169395447
Batch loss: 0.5028294324874878
Batch loss: 1.1457736492156982
Batch loss: 3.3170430660247803
Batch loss: 3.7823474407196045
Batch loss: 0.9270302653312683
Batch loss: 2.300811767578125
Batch loss: 74.18414306640625
Batch loss: 18.081932067871094
Batch loss: 1.3795506954193115
Batch loss: 3.0181379318237305
Batch loss: 0.997663676738739
Batch loss: 5.6391520500183105
Batch loss: 25.329444885253906
Batch loss: 3.470024585723877
Batch loss: 3.978656768798828
Batch loss: 6.673076629638672
Batch loss: 2.347717761993408
Batch loss: 13.276265144348145
Batch loss: 2.411156415939331
Batch loss: 0.7203836441040039
Batch loss: 8.613226890563965
Batch loss: 2.088056802749634
Batch loss: 3.507716417312622
Batch loss: 0.7508487105369568
Batch loss: 0.9328458905220032
Batch loss: -0.6192418932914734
Batch loss: 2.138033628463745
Batch loss: 1.2310659885406494
Batch loss: -0.293487012386322
Batch loss: 1.87039315700531
Batch loss: 0.12293899059295654
Batch loss: 0.794084906578064
Batch loss: 1.4923007488250732
Batch loss: 1.2923544645309448
Batch loss: 7.978628635406494
Batch loss: 0.40776005387306213
Batch loss: 1.4844790697097778
Batch loss: 1.5922073125839233
Batch loss: 0.6069231033325195
Batch loss: 2.0644350051879883
Batch loss: 7.973460674285889
Batch loss: 1.3271105289459229
Batch loss: 1.0178886651992798
Batch loss: 16.702123641967773
Batch loss: 1.351946473121643
Batch loss: 3.0588364601135254
Batch loss: 1.301074504852295
Batch loss: 0.38594746589660645
Batch loss: 1.8136978149414062
Batch loss: 0.27217647433280945
Batch loss: 1.0872300863265991
Batch loss: 3.426307439804077
Epoch: 2 	Training Loss: 5.344870
Batch loss: 0.761509358882904
Batch loss: 6.82588529586792
Batch loss: 40.179622650146484
Batch loss: 0.2891926169395447
Batch loss: 0.5028294324874878
Batch loss: 1.1457736492156982
Batch loss: 3.3170430660247803
Batch loss: 3.7823474407196045
Batch loss: 0.9270302653312683
Batch loss: 2.300811767578125
Batch loss: 74.18414306640625
Batch loss: 18.081932067871094
Batch loss: 1.3795506954193115
Batch loss: 3.0181379318237305
Batch loss: 0.997663676738739
Batch loss: 5.6391520500183105
Batch loss: 25.329444885253906
Batch loss: 3.470024585723877
Batch loss: 3.978656768798828
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 283, in <module>
    train_loop(llm)
  File "/home/franlucc/projects/codetrace/train.py", line 204, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 80, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 73, in __exit__
    self.run_local()
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 78, in run_local
    self.output = self.model(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 194, in __call__
    output = fn(inputs, *args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/LanguageModel.py", line 166, in _generation
    return super()._generation(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 412, in _generation
    return self.local_model.generate(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 1265, in forward
    transformer_outputs = self.transformer(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 1118, in forward
    outputs = block(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/hooks.py", line 297, in pre_forward
    return send_to_device(args, self.execution_device), send_to_device(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/utils/operations.py", line 161, in send_to_device
    {
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/utils/operations.py", line 162, in <dictcomp>
    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/accelerate/utils/operations.py", line 171, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
KeyboardInterrupt