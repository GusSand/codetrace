264
216
Training...
Batch loss: 0.6596150398254395
Batch loss: 0.3502698242664337
Batch loss: 0.31102100014686584
Batch loss: 0.307352751493454
Batch loss: 0.3068496882915497
Batch loss: 0.35427066683769226
Batch loss: 0.5186729431152344
Batch loss: 0.47183915972709656
Batch loss: 0.4210514724254608
Batch loss: 0.3309101462364197
Batch loss: 0.36305180191993713
Batch loss: 0.3566276729106903
Batch loss: 0.5837186574935913
Batch loss: 0.3868774473667145
Batch loss: 0.4706576466560364
Batch loss: 0.5464938282966614
Batch loss: 0.6050467491149902
Batch loss: 0.4424155652523041
Batch loss: 0.48688387870788574
Batch loss: 0.2992541491985321
Batch loss: 0.5028147101402283
Batch loss: 0.3974546790122986
Batch loss: 0.553936779499054
Batch loss: 0.2757962644100189
Batch loss: 0.2088562548160553
Batch loss: 0.36341220140457153
Batch loss: 0.31768956780433655
Batch loss: 0.27649784088134766
Batch loss: 0.3764587342739105
Batch loss: 0.4992027282714844
Batch loss: 0.6943766474723816
Batch loss: 0.49227219820022583
Batch loss: 0.35799214243888855
Batch loss: 0.34778153896331787
Batch loss: 0.6172997355461121
Batch loss: 0.6215017437934875
Batch loss: 0.2896690368652344
Batch loss: 0.41803017258644104
Batch loss: 0.42187070846557617
Batch loss: 0.37616294622421265
Batch loss: 0.4891057014465332
Batch loss: 0.37730130553245544
Batch loss: 0.5190445184707642
Epoch: 1 	Training Loss: 0.417441
Batch loss: 0.6326723098754883
Batch loss: 0.3519669473171234
Batch loss: 0.2451261579990387
Batch loss: 0.26949772238731384
Batch loss: 0.27616778016090393
Batch loss: 0.4515255093574524
Batch loss: 0.49789267778396606
Batch loss: 0.4369508922100067
Batch loss: 0.42000970244407654
Batch loss: 0.34970518946647644
Batch loss: 0.44838571548461914
Batch loss: 0.3550552427768707
Batch loss: 0.5343371629714966
Batch loss: 0.388388454914093
Batch loss: 0.46068087220191956
Batch loss: 0.492520809173584
Batch loss: 0.5704245567321777
Batch loss: 0.4433487355709076
Batch loss: 0.4397915005683899
Batch loss: 0.3013435900211334
Batch loss: 0.5854899287223816
Batch loss: 0.39384326338768005
Batch loss: 0.489195317029953
Batch loss: 0.2873896658420563
Batch loss: 0.32427915930747986
Batch loss: 0.3476335406303406
Batch loss: 0.44158920645713806
Batch loss: 0.3472200632095337
Batch loss: 0.3599592447280884
Batch loss: 0.5345299243927002
Batch loss: 0.7178124785423279
Batch loss: 0.5339998602867126
Batch loss: 0.315791517496109
Batch loss: 0.36737868189811707
Batch loss: 0.6277429461479187
Batch loss: 0.5814847350120544
Batch loss: 0.3758111596107483
Batch loss: 0.3612792193889618
Batch loss: 0.4195791184902191
Batch loss: 0.49856534600257874
Batch loss: 0.4336525499820709
Batch loss: 0.47346583008766174
Batch loss: 0.5101101994514465
Epoch: 2 	Training Loss: 0.424854
Batch loss: 0.6821747422218323
Batch loss: 0.3218437135219574
Batch loss: 0.31268760561943054
Batch loss: 0.2577340304851532
Batch loss: 0.3111509084701538
Batch loss: 0.4249250888824463
Batch loss: 0.49314555525779724
Batch loss: 0.4826638698577881
Batch loss: 0.4284948408603668
Batch loss: 0.29073795676231384
Batch loss: 0.4497144818305969
Batch loss: 0.30005356669425964
Batch loss: 0.5115568041801453
Batch loss: 0.4007973372936249
Batch loss: 0.48611560463905334
Batch loss: 0.43737077713012695
Batch loss: 0.5862550139427185
Batch loss: 0.4084908664226532
Batch loss: 0.5375102162361145
Batch loss: 0.2898068428039551
Batch loss: 0.566593587398529
Batch loss: 0.30051377415657043
Batch loss: 0.4638671576976776
Batch loss: 0.370534211397171
Batch loss: 0.3540409803390503
Batch loss: 0.35486099123954773
Batch loss: 0.4561346173286438
Batch loss: 0.2770490348339081
Batch loss: 0.30909866094589233
Batch loss: 0.5592684745788574
Batch loss: 0.7248331904411316
Batch loss: 0.5242806077003479
Batch loss: 0.3238605558872223
Batch loss: 0.3934253752231598
Batch loss: 0.6571247577667236
Batch loss: 0.5331898927688599
Batch loss: 0.20116202533245087
Batch loss: 0.4347549378871918
Batch loss: 0.3911344110965729
Batch loss: 0.44629907608032227
Batch loss: 0.47800469398498535
Batch loss: 0.4286755621433258
Batch loss: 0.5249232053756714
Epoch: 3 	Training Loss: 0.420156
Batch loss: 0.6391846537590027
Batch loss: 0.3055942952632904
Batch loss: 0.35515519976615906
Batch loss: 0.2285900115966797
Batch loss: 0.3751799166202545
Batch loss: 0.35350510478019714
Batch loss: 0.5305577516555786
Batch loss: 0.5271207094192505
Batch loss: 0.4279744625091553
Batch loss: 0.261434942483902
Batch loss: 0.35859763622283936
Batch loss: 0.3105340600013733
Batch loss: 0.4796806275844574
Batch loss: 0.3734560012817383
Batch loss: 0.49220696091651917
Batch loss: 0.5058364272117615
Batch loss: 0.5848997235298157
Batch loss: 0.5137964487075806
Batch loss: 0.5231391191482544
Batch loss: 0.24143090844154358
Batch loss: 0.5595437288284302
Batch loss: 0.32703524827957153
Batch loss: 0.49484261870384216
Batch loss: 0.29642996191978455
Batch loss: 0.29279839992523193
Batch loss: 0.3448927104473114
Batch loss: 0.28263965249061584
Batch loss: 0.33855873346328735
Batch loss: 0.38087591528892517
Batch loss: 0.5878812670707703
Batch loss: 0.6570048332214355
Batch loss: 0.4864208400249481
Batch loss: 0.3101412355899811
Batch loss: 0.267999529838562
Batch loss: 0.638996422290802
Batch loss: 0.5553550124168396
Batch loss: 0.2660381495952606
Batch loss: 0.4677935242652893
Batch loss: 0.41102394461631775
Batch loss: 0.482173353433609
Batch loss: 0.5380746126174927
Batch loss: 0.38534608483314514
Batch loss: 0.5168086886405945
Epoch: 4 	Training Loss: 0.415376
Batch loss: 0.6580018401145935
Batch loss: 0.3168581426143646
Batch loss: 0.32499799132347107
Batch loss: 0.2562456727027893
Batch loss: 0.3056463301181793
Batch loss: 0.37335866689682007
Batch loss: 0.5508227944374084
Batch loss: 0.420577734708786
Batch loss: 0.41625022888183594
Batch loss: 0.4081279933452606
Batch loss: 0.4817858636379242
Batch loss: 0.34039366245269775
Batch loss: 0.49352145195007324
Batch loss: 0.39327797293663025
Batch loss: 0.4411207139492035
Batch loss: 0.487215131521225
Batch loss: 0.5949285626411438
Batch loss: 0.3844732642173767
Batch loss: 0.4963156282901764
Batch loss: 0.35779815912246704
Batch loss: 0.49894553422927856
Batch loss: 0.3633705973625183
Batch loss: 0.45976802706718445
Batch loss: 0.33216577768325806
Batch loss: 0.3952677845954895
Batch loss: 0.30601564049720764
Batch loss: 0.3633268475532532
Batch loss: 0.2770632803440094
Batch loss: 0.35949063301086426
Batch loss: 0.5908141136169434
Batch loss: 0.6908429265022278
Batch loss: 0.48099955916404724
Batch loss: 0.3154764175415039
Batch loss: 0.3619125783443451
Batch loss: 0.6350356340408325
Batch loss: 0.5689458250999451
Batch loss: 0.26290255784988403
Batch loss: 0.5003933310508728
Batch loss: 0.41831374168395996
Batch loss: 0.4818570613861084
Batch loss: 0.4779118597507477
Batch loss: 0.3534397780895233
Batch loss: 0.4705250859260559
Epoch: 5 	Training Loss: 0.419693
Batch loss: 0.6829720735549927
Batch loss: 0.32629743218421936
Batch loss: 0.38085219264030457
Batch loss: 0.2622487246990204
Batch loss: 0.30528098344802856
Batch loss: 0.40307632088661194
Batch loss: 0.4781173765659332
Batch loss: 0.36726218461990356
Batch loss: 0.4209364056587219
Batch loss: 0.36904460191726685
Batch loss: 0.42674025893211365
Batch loss: 0.36571618914604187
Batch loss: 0.46467193961143494
Batch loss: 0.3467429578304291
Batch loss: 0.47409388422966003
Batch loss: 0.4796693027019501
Batch loss: 0.6188089847564697
Batch loss: 0.46622785925865173
Batch loss: 0.5097079277038574
Batch loss: 0.28644049167633057
Batch loss: 0.5607873201370239
Batch loss: 0.3078064024448395
Batch loss: 0.4964299201965332
Batch loss: 0.2422776073217392
Batch loss: 0.31115955114364624
Batch loss: 0.3230345547199249
Batch loss: 0.38633108139038086
Batch loss: 0.2969890534877777
Batch loss: 0.3579340875148773
Batch loss: 0.5810532569885254
Batch loss: 0.6885170936584473
Batch loss: 0.4736858010292053
Batch loss: 0.33880576491355896
Batch loss: 0.30634334683418274
Batch loss: 0.6689338684082031
Batch loss: 0.6123992204666138
Batch loss: 0.24553759396076202
Batch loss: 0.4854643940925598
Batch loss: 0.4463636577129364
Batch loss: 0.49404221773147583
Batch loss: 0.4864688813686371
Batch loss: 0.3941269814968109
Batch loss: 0.49980592727661133
Epoch: 6 	Training Loss: 0.419073
Batch loss: 0.6687406301498413
Batch loss: 0.29199057817459106
Batch loss: 0.2601642310619354
Batch loss: 0.22383056581020355
Batch loss: 0.32470938563346863
Batch loss: 0.3497478663921356
Batch loss: 0.5206084251403809
Batch loss: 0.5447427034378052
Batch loss: 0.413185179233551
Batch loss: 0.3535442352294922
Batch loss: 0.47912508249282837
Batch loss: 0.26314958930015564
Batch loss: 0.5308998227119446
Batch loss: 0.39792683720588684
Batch loss: 0.48045405745506287
Batch loss: 0.47304898500442505
Batch loss: 0.6332147717475891
Batch loss: 0.4002927839756012
Batch loss: 0.49578171968460083
Batch loss: 0.2999216616153717
Batch loss: 0.5757171511650085
Batch loss: 0.3758712708950043
Batch loss: 0.5080875754356384
Batch loss: 0.37897858023643494
Batch loss: 0.33562859892845154
Batch loss: 0.2986813485622406
Batch loss: 0.2775239646434784
Batch loss: 0.24839232861995697
Batch loss: 0.39934515953063965
Batch loss: 0.601512610912323
Batch loss: 0.6960746645927429
Batch loss: 0.4788881838321686
Batch loss: 0.31819960474967957
Batch loss: 0.30679258704185486
Batch loss: 0.6107518672943115
Batch loss: 0.5599440932273865
Batch loss: 0.21452264487743378
Batch loss: 0.4243393838405609
Batch loss: 0.4610874354839325
Batch loss: 0.4812373220920563
Batch loss: 0.40568026900291443
Batch loss: 0.3846982419490814
Batch loss: 0.49702492356300354
Epoch: 7 	Training Loss: 0.414638
Batch loss: 0.5763064026832581
Batch loss: 0.29258015751838684
Batch loss: 0.2758105397224426
Batch loss: 0.15781696140766144
Batch loss: 0.30981263518333435
Batch loss: 0.37496766448020935
Batch loss: 0.4824232757091522
Batch loss: 0.46607574820518494
Batch loss: 0.40299710631370544
Batch loss: 0.3424862325191498
Batch loss: 0.38794419169425964
Batch loss: 0.3656041622161865
Batch loss: 0.51519376039505
Batch loss: 0.4093452990055084
Batch loss: 0.4131077229976654
Batch loss: 0.4596303105354309
Batch loss: 0.5835272073745728
Batch loss: 0.38628003001213074
Batch loss: 0.498877614736557
Batch loss: 0.29945921897888184
Batch loss: 0.5556120276451111
Batch loss: 0.30646905303001404
Batch loss: 0.5112586617469788
Batch loss: 0.26448699831962585
Batch loss: 0.3598063886165619
Batch loss: 0.374918133020401
Batch loss: 0.38380035758018494
Batch loss: 0.3783014416694641
Batch loss: 0.3273891508579254
Batch loss: 0.5323545336723328
Batch loss: 0.7063484191894531
Batch loss: 0.533689558506012
Batch loss: 0.32127222418785095
Batch loss: 0.2996857464313507
Batch loss: 0.6421260237693787
Batch loss: 0.6304252743721008
Batch loss: 0.283207505941391
Batch loss: 0.37467923760414124
Batch loss: 0.4728623032569885
Batch loss: 0.464809387922287
Batch loss: 0.46334075927734375
Batch loss: 0.3475017249584198
Batch loss: 0.49570971727371216
Epoch: 8 	Training Loss: 0.409780
Batch loss: 0.6756231188774109
Batch loss: 0.28272005915641785
Batch loss: 0.34286442399024963
Batch loss: 0.19802068173885345
Batch loss: 0.27035951614379883
Batch loss: 0.39819690585136414
Batch loss: 0.5512955784797668
Batch loss: 0.509054958820343
Batch loss: 0.46582087874412537
Batch loss: 0.3592260777950287
Batch loss: 0.37729573249816895
Batch loss: 0.3026026785373688
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 275, in <module>
    train_loop(llm)
  File "/home/franlucc/projects/codetrace/train.py", line 196, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 79, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 73, in __exit__
    self.run_local()
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 78, in run_local
    self.output = self.model(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 194, in __call__
    output = fn(inputs, *args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/LanguageModel.py", line 166, in _generation
    return super()._generation(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 412, in _generation
    return self.local_model.generate(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 2392, in greedy_search
    if unfinished_sequences.max() == 0:
KeyboardInterrupt