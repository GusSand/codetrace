392
278
Training...
Batch loss: 0.6555279493331909
Batch loss: -0.12091930210590363
Batch loss: -0.07319233566522598
Batch loss: -0.3503030836582184
Batch loss: -0.29414108395576477
Batch loss: 0.16082224249839783
Batch loss: 1.4806149005889893
Batch loss: -0.4307347238063812
Batch loss: 0.6949231028556824
Batch loss: 0.8149663805961609
Batch loss: 0.9115458726882935
Batch loss: 0.32859307527542114
Batch loss: -0.1803591102361679
Batch loss: -0.46906405687332153
Batch loss: 0.6542244553565979
Batch loss: -0.24385853111743927
Batch loss: -0.08072509616613388
Batch loss: 3.894716262817383
Batch loss: 0.7408038973808289
Batch loss: 0.9472530484199524
Batch loss: -0.09956357628107071
Batch loss: 0.7122507095336914
Batch loss: 0.23369057476520538
Batch loss: 0.041458602994680405
Batch loss: 2.09468150138855
Batch loss: -0.37806469202041626
Batch loss: 1.6829522848129272
Batch loss: 0.049796056002378464
Batch loss: 2.4708778858184814
Batch loss: 0.6964014172554016
Batch loss: -0.27599215507507324
Batch loss: 0.7445957660675049
Batch loss: 0.5441883206367493
Batch loss: -0.3287259042263031
Batch loss: 0.32636353373527527
Batch loss: -0.461050808429718
Batch loss: -0.5178110003471375
Batch loss: -0.3215322196483612
Batch loss: 0.011497325263917446
Batch loss: 0.9722520709037781
Batch loss: 0.4812951982021332
Batch loss: 0.8062982559204102
Batch loss: 1.8565715551376343
Batch loss: -0.3491195738315582
Batch loss: -0.4933694303035736
Batch loss: 0.7353389263153076
Batch loss: 1.097895860671997
Batch loss: -0.06656618416309357
Batch loss: -0.39832744002342224
Batch loss: 1.6574970483779907
Batch loss: 1.7610288858413696
Batch loss: -0.1591840237379074
Batch loss: 0.40727224946022034
Batch loss: -0.01718093827366829
Batch loss: -0.08912569284439087
Batch loss: 0.45865583419799805
Epoch: 1 	Training Loss: 0.445142
Batch loss: 0.6555279493331909
Batch loss: -0.12091930210590363
Batch loss: -0.07319233566522598
Batch loss: -0.3503030836582184
Batch loss: -0.29414108395576477
Batch loss: 0.16082224249839783
Batch loss: 1.4806149005889893
Batch loss: -0.4307347238063812
Traceback (most recent call last):
  File "/home/franlucc/projects/codetrace/train.py", line 283, in <module>
    train_loop(llm)
  File "/home/franlucc/projects/codetrace/train.py", line 204, in train_loop
    causal_loss, _ = apply_mask(llm, output, prompts, correct_idxs, incorrect_idxs)
  File "/home/franlucc/projects/codetrace/train.py", line 80, in apply_mask
    with llm.generate(max_new_tokens=1,
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 73, in __exit__
    self.run_local()
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/contexts/Runner.py", line 78, in run_local
    self.output = self.model(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 194, in __call__
    output = fn(inputs, *args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/LanguageModel.py", line 166, in _generation
    return super()._generation(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/nnsight/models/NNsightModel.py", line 412, in _generation
    return self.local_model.generate(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
  File "/home/franlucc/venvs/gpu/lib/python3.10/site-packages/transformers/generation/utils.py", line 2392, in greedy_search
    if unfinished_sequences.max() == 0:
KeyboardInterrupt