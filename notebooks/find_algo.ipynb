{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation patching basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cuda:0\"\n",
    "starcoderbase_1b = \"/home/arjun/models/starcoderbase-1b/\"\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from src.utils import *\n",
    "import datasets\n",
    "from src.experiments.wrong_type_patching import *\n",
    "from src.experiments.type_patching import *\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import plotly.express as px\n",
    "from src.patching import *\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from src.utils import *\n",
    "import regex as re\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"causal_mask_epoch_1.pt\",\"rb\") as f:\n",
    "    causal_mask = torch.load(f)\n",
    "    \n",
    "print(causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(starcoderbase_1b, device_map=device)\n",
    "model, model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits for inf binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"franlucc/ts_bench_starcoder1b_funcfim_incorrect_uniq_v1\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "seen = set()\n",
    "model.tokenizer.pad_token_id = model.tokenizer.eos_token_id\n",
    "correct = []\n",
    "incorrect = []\n",
    "\n",
    "for i,ex in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    if i > 10:\n",
    "        break\n",
    "    prompt = ex[\"prompt\"]\n",
    "    if prompt in seen:\n",
    "        print(\"Skipping \", i)\n",
    "        continue\n",
    "    seen.add(prompt)\n",
    "    def decoder(x):\n",
    "        # decoder consists of final layer norm + unembedding\n",
    "        return model.lm_head(model.transformer.ln_f(x))\n",
    "\n",
    "    logits_before = []\n",
    "    logits = []\n",
    "    with model.generate(max_new_tokens=2) as generator:\n",
    "        with generator.invoke(prompt) as invoker:\n",
    "            tokens = invoker.input[\"input_ids\"]\n",
    "            for l in model.transformer.h:\n",
    "                logits_before.append(decoder(l.output[0]).save())\n",
    "            invoker.next()\n",
    "\n",
    "            for l in model.transformer.h:\n",
    "                logits.append(decoder(l.output[0]).save())\n",
    "                \n",
    "\n",
    "    logits = util.apply(logits, lambda x: x.value.cpu(), Proxy)\n",
    "    logits_before = util.apply(logits_before, lambda x: x.value.cpu(), Proxy)\n",
    "\n",
    "    logits = [torch.cat((a, b), dim=1) for (a,b) in zip(logits_before, logits)]\n",
    "    logits = [l.cpu().detach().softmax(-1) for l in logits]\n",
    "\n",
    "    tokens = torch.cat((tokens[0], logits[-1][:,-2,:].argmax(-1))) #-2 here\n",
    "    \n",
    "    def to_string(x):\n",
    "        s =  model.tokenizer.decode([x])\n",
    "        if s in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<fim_prefix>\", \"<fim_suffix>\", \"<fim_middle>\"]:\n",
    "            s = \"*\" + s[1:-1] + \"*\"\n",
    "        return s\n",
    "    \n",
    "    l = 18\n",
    "\n",
    "    for e,tok in enumerate(tokens):\n",
    "        if tok == model.tokenizer.encode(\"<fim_suffix>\")[0]:\n",
    "            max_logit = logits[l][:,e-1,:].max(-1)\n",
    "            max_logit_i = max_logit.indices.item()\n",
    "            max_logit_v = max_logit.values.item()\n",
    "            max_logit_i = model.tokenizer.decode(max_logit_i)\n",
    "            if int(max_logit_i) == int(ex[\"fim_sol\"]):\n",
    "                print(\"Correct idx\", i)\n",
    "                correct.append((i, max_logit_v))\n",
    "                count += 1\n",
    "            else:\n",
    "                max_logit = logits[l][:,-2,:].max(-1)\n",
    "                max_logit_i = max_logit.indices.item()\n",
    "                max_logit_v = max_logit.values.item()\n",
    "                max_logit_i = model.tokenizer.decode(max_logit_i)\n",
    "                if int(max_logit_i) == int(ex[\"fim_sol\"]):\n",
    "                    print(\"Correct idx\", i)\n",
    "                    correct.append((i, max_logit_v, -1))\n",
    "                    print(\"Correct idx mid\", i)\n",
    "                else:\n",
    "                    incorrect.append((i, max_logit_v, max_logit_i, logits[l][:,e-1,model.tokenizer.encode(ex[\"fim_sol\"])[0]].item()))\n",
    "                    print(\"Incorrect idx\", i)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect), len(correct), len(correct) / (len(incorrect) + len(correct)), len(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Code\n",
    "from circuitsvis.logits import token_log_probs\n",
    "k= -2\n",
    "i = incorrect[k][0]\n",
    "prompt_fim = dataset[i][\"prompt\"]\n",
    "print(\"correct\", dataset[i][\"fim_sol\"], \" incorrect\", dataset[i][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(prompt_fim, language=\"typescript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "  # decoder consists of final layer norm + unembedding\n",
    "  return model.lm_head(model.transformer.ln_f(x))\n",
    "\n",
    "logits_before = []\n",
    "logits = []\n",
    "with model.generate(max_new_tokens=2) as generator:\n",
    "    with generator.invoke(prompt_fim) as invoker:\n",
    "        tokens = invoker.input[\"input_ids\"]\n",
    "        for l in model.transformer.h:\n",
    "            logits_before.append(decoder(l.output[0]).save())\n",
    "        invoker.next()\n",
    "\n",
    "        for l in model.transformer.h:\n",
    "            logits.append(decoder(l.output[0]).save())\n",
    "            \n",
    "\n",
    "logits = util.apply(logits, lambda x: x.value.cpu(), Proxy)\n",
    "logits_before = util.apply(logits_before, lambda x: x.value.cpu(), Proxy)\n",
    "\n",
    "print(logits[0].shape, logits_before[0].shape)\n",
    "\n",
    "logits = [torch.cat((a, b), dim=1) for (a,b) in zip(logits_before, logits)]\n",
    "logits = [l.cpu().detach().softmax(-1) for l in logits]\n",
    "print(logits[-1].shape)\n",
    "print([to_string(t) for t in tokens[0]])\n",
    "tokens = torch.cat((tokens[0], logits[-1][:,-2,:].argmax(-1))) #-2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(x):\n",
    "    s =  model.tokenizer.decode([x])\n",
    "    if s in [\"<fim_prefix>\", \"<fim_suffix>\", \"<fim_middle>\"]:\n",
    "        s = \"*\" + s[1:-1] + \"*\"\n",
    "    return s\n",
    "  \n",
    "l = 16\n",
    "print([to_string(t) for t in tokens])\n",
    "print(\"correct\", dataset[i][\"fim_sol\"], \" incorrect\", dataset[i][\"generated_text\"])\n",
    "for e,tok in enumerate(tokens):\n",
    "    if tok == model.tokenizer.encode(\"<fim_suffix>\")[0]:\n",
    "        max_logit = logits[l][:,e-1,:].max(-1)\n",
    "        max_logit_v = max_logit.values.item()\n",
    "        max_logit = model.tokenizer.decode(max_logit.indices.item())\n",
    "        print(max_logit, max_logit_v)\n",
    "        # if int(max_logit) == int(dataset[i][\"fim_sol\"]):\n",
    "        #     print(\"Correct idx\", i)\n",
    "        #     count += 1\n",
    "    \n",
    "\n",
    "# token_log_probs(tokens, torch.log(logits[l]), to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating what happens when a model gives the wrong prediction\n",
    "\n",
    "Here we give an approximate algorithm of what the model may be doing:\n",
    "\n",
    "1. identify first untyped variable and bind it to a value of the same type by transfering info from value to variable\n",
    "2. repeat 1 until all variables are typed\n",
    "3. copy the answer onto `<fim_middle>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"franlucc/starcoderbase-1b-wrong-type-patching-v0-seed42\", split=\"train\")\n",
    "working_ds = ds.filter(lambda x: x[\"success\"])\n",
    "broken_ds = ds.filter(lambda x: not x[\"success\"])\n",
    "ds, working_ds, broken_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at one failing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Code\n",
    "content = broken_ds[0][\"prompt\"]\n",
    "solution = broken_ds[0][\"solution\"]\n",
    "generated = broken_ds[0][\"generated\"]\n",
    "print(solution, \"\\n\\n\", generated)\n",
    "content = content.split(\"function _uniq_7\")[0]\n",
    "\n",
    "rename = {\n",
    "    \"_uniq_0\": \"_uniq_q\",\n",
    "    \"_uniq_1[^\\d]\": \"_uniq_t\",\n",
    "}\n",
    "def rename_vars(content, rename):\n",
    "    for old, new in rename.items():\n",
    "        content = re.sub(old, new, content)\n",
    "    return content\n",
    "\n",
    "content = rename_vars(content, rename)\n",
    "solution = rename_vars(solution, rename)\n",
    "generated = rename_vars(generated, rename)\n",
    "print(\"SOLUTION:\", solution, \"\\n\\n\", \"GENERATED:\", generated)\n",
    "\n",
    "code = Code(content, language=\"typescript\")\n",
    "base_prompt = placeholder_to_std_fmt(content, STARCODER_FIM)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"function\" +base_prompt.split(\"function\")[-1]\n",
    "\n",
    "max_new_tokens = 4\n",
    "\n",
    "def get_final_states(prompt, max_new_tokens):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            hidden_states = []\n",
    "            for i in range(max_new_tokens):\n",
    "                final_hs = model.lm_head.output\n",
    "                print(model.transformer.h[5].mlp.output.shape)\n",
    "                hidden_states.append(final_hs.save())\n",
    "                invoker.next()\n",
    "    return hidden_states\n",
    "\n",
    "hidden_states = get_final_states(prompt, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states]\n",
    "\n",
    "topk=10\n",
    "predictions = []\n",
    "for j,hs in enumerate(out):\n",
    "    if j == 0:\n",
    "        hs = hs[:, -1, :]\n",
    "        hs = hs.unsqueeze(1)\n",
    "    top_vals_and_idx = hs.softmax(dim=-1).topk(topk)\n",
    "    indices = top_vals_and_idx.indices[0][0].tolist()\n",
    "    values = top_vals_and_idx.values.tolist()[0][0]\n",
    "    top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "    predictions.append(list(zip(top_idx, values)))\n",
    "\n",
    "greedy_prediction = \"\".join([p[0][0] for p in predictions])\n",
    "print(greedy_prediction)\n",
    "result_table = pd.DataFrame(predictions, columns=[f\"top_pred_{i}\" for i in range(topk)], index=[f\"next_token_{i}\" for i in range(max_new_tokens)])\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check generation\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(starcoderbase_1b)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(starcoderbase_1b).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens = 4\n",
    "# tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "# out = llm.generate(tokens, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "# text = [tokenizer.decode(i) for i in out.tolist()]\n",
    "# text = \"\".join(text)\n",
    "# text.split(\"<fim_middle>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_tokens = []\n",
    "def get_all_hidden_states(prompt, max_new_tokens):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            saved_tokens.append(invoker.input.tokens())\n",
    "            print(saved_tokens)\n",
    "            hidden_states = {layer : [] for layer in range(model.config.n_layer)}\n",
    "            for i in range(max_new_tokens-1):\n",
    "                invoker.next()\n",
    "                \n",
    "            for l in range(model.config.n_layer):\n",
    "                hs = model.transformer.h[l].output[0].save()\n",
    "                hidden_states[l].append(model.lm_head(model.transformer.ln_f(hs)).save())\n",
    "    return hidden_states\n",
    "\n",
    "full_hidden_states = get_all_hidden_states(prompt, max_new_tokens)\n",
    "value_hidden_states = {k : [util.apply(i, lambda x: x.value, Proxy) for i in v] for k,v in full_hidden_states.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_preds = {layer : [] for layer in range(model.config.n_layer)}\n",
    "# value_hidden_states[14]\n",
    "for layer, hs in value_hidden_states.items():\n",
    "    tok_hs = hs[0]\n",
    "    top_vals_and_idx = tok_hs.softmax(dim=-1).topk(topk)\n",
    "    indices = top_vals_and_idx.indices.tolist()[0][0]\n",
    "    values = top_vals_and_idx.values.tolist()[0][0]\n",
    "    top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "    tok_preds[layer].append(list(zip(top_idx, values)))\n",
    "    \n",
    "tok_preds_df = pd.DataFrame(tok_preds)\n",
    "tok_preds_df.columns = [f\"layer_{i}\" for i in range(model.config.n_layer)]\n",
    "tok_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at logits in prompt and how they change through layers\n",
    "# for each layer, collect the topk predictions for each token in the prompt\n",
    "\n",
    "prompt_tok_preds = {l : [] for l in range(model.config.n_layer)}\n",
    "for layer, hs in value_hidden_states.items():\n",
    "    prompt_hs = hs[0]\n",
    "    for k in range(prompt_hs.shape[1]):\n",
    "        tok_hs = prompt_hs[:, k, :]\n",
    "        top_vals_and_idx = tok_hs.softmax(dim=-1).topk(topk)\n",
    "        indices = top_vals_and_idx.indices[0].tolist()\n",
    "        values = top_vals_and_idx.values.tolist()[0]\n",
    "        top_idx = [model.tokenizer.decode(i) for i in indices]\n",
    "        prompt_tok_preds[layer].append(list(zip(top_idx, values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_logits = pd.DataFrame(prompt_tok_preds)\n",
    "prompt_logits.columns = [f\"layer_{i}\" for i in range(model.config.n_layer)]\n",
    "prompt_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_layer = prompt_logits[\"layer_23\"].tolist()\n",
    "# print(len(curr_layer))\n",
    "\n",
    "# probabilities = []\n",
    "# text = []\n",
    "# x = []\n",
    "# minn = 251\n",
    "# for k in range(minn, minn+10):\n",
    "#     token = curr_layer[k]\n",
    "#     token_logit_text = [i[0] for i in token]\n",
    "#     token_logit_prob = [i[1] for i in token]\n",
    "#     probabilities.append(token_logit_prob)\n",
    "#     x.append(saved_tokens[0][k])\n",
    "#     text.append(token_logit_text)\n",
    "    \n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#                     z=probabilities,\n",
    "#                     x = x,\n",
    "#                     text=text,\n",
    "#                     texttemplate=\"%{text}\",\n",
    "#                     textfont={\"size\":15}))\n",
    "\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons why prediction is wrong\n",
    "- 0 is similar to 1 (number issue)\n",
    "    - probably not, can change 0 to q and 1 to t and it still fails\n",
    "- another algorithm picks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"====Generated:====\\n\\n{generated}\\n\\n====Solution:=====\\n\\n{solution}\")\n",
    "code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attn visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO something wrong here, always gives same resd\n",
    "attn, toks = attention_vis(model, prompt+\"_uniq_\")\n",
    "print(len(attn), torch.stack(attn).sum().item())\n",
    "# b, h, q, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def normalize_attn(attention_matrix):\n",
    "    return normalize(attention_matrix, norm=\"l2\", axis=1)\n",
    "\n",
    "print(attn[0].shape)\n",
    "normalize_attn(attn[13][3].cpu().detach().numpy()), attn[13][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.attention.attention_pattern(tokens=no_special_tok[window_idx:], attention=attn[14][window_idx:], mask_upper_tri=True)\n",
    "# query is destination, key is source\n",
    "\n",
    "# heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "def create_attention_heatmap(attention_matrix, tokens, row_idx, column_idx, step=10):\n",
    "    normalized_attention = normalize_attn(attention_matrix)\n",
    "    \n",
    "    # Get a subset of the attention matrix\n",
    "    subset_matrix = normalized_attention[row_idx:row_idx+step, column_idx:column_idx+step]\n",
    "    xtokens = tokens[column_idx:column_idx+step]\n",
    "    ytokens = tokens[row_idx:row_idx+step]\n",
    "\n",
    "    \n",
    "    # Create a heatmap using Seaborn\n",
    "    sns.set()\n",
    "\n",
    "    sns.heatmap(subset_matrix, cmap=\"viridis\", xticklabels=xtokens, yticklabels=ytokens)\n",
    "    # annot=subset_matrix, fmt=\".2f\", annot_kws={\"size\": 10}\n",
    "    \n",
    "    plt.xlabel(\"Quert DST Tokens\")\n",
    "    plt.ylabel(\"Key SRC Tokens\")\n",
    "\n",
    "\n",
    "\n",
    "layers = []\n",
    "heads = list(range(16))\n",
    "window_range = list(range(0, len(toks)-10, 10))\n",
    "\n",
    "column_idx = len(toks)-10\n",
    "step = 10\n",
    "full_sum = 0\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "    \n",
    "for layer in tqdm(layers):\n",
    "    os.makedirs(f\"images/layer_{layer}\", exist_ok=True)\n",
    "\n",
    "    for row_idx in window_range:\n",
    "        xtokens = {i:t for i,t in enumerate(toks[column_idx:column_idx+step])}\n",
    "        ytokens = {i:t for i,t in enumerate(toks[row_idx:row_idx+step])}\n",
    "        # print(\"X DST:\\n\",xtokens, \"\\nY SRC:\\n\", ytokens)\n",
    "        for h in heads:\n",
    "            # create a subplot for this head    \n",
    "            plt.subplot(4,4, h+1)\n",
    "            kq = attn[layer][h].cpu().detach().numpy()\n",
    "            \n",
    "            summ = kq[row_idx:row_idx+step, column_idx:column_idx+step].sum().item()\n",
    "            if summ == 0:\n",
    "                continue\n",
    "            full_sum += summ\n",
    "            create_attention_heatmap(kq, toks, row_idx=row_idx, column_idx=column_idx, step=step)\n",
    "        \n",
    "        if full_sum > 0:\n",
    "            # print(f\"Saving {h} head at row {row_idx}\")\n",
    "            full_sum = 0\n",
    "            # save\n",
    "            plt.tight_layout()\n",
    "            with open(f\"images/layer_{layer}/row_{row_idx}_column_{column_idx}_step_{step}.txt\", \"w\") as f:\n",
    "                f.write(f\"X DST:\\n{xtokens}\\nY SRC:\\n{ytokens}\")\n",
    "            plt.savefig(f\"images/layer_{layer}/row_{row_idx}_column_{column_idx}_step_{step}.png\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer erase - logit difference\n",
    "\n",
    "erasing doesn't quite work -> maybe try noise or random patch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_states_erase(prompt, max_new_tokens, layers = []):\n",
    "    with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
    "        with generator.invoke(input=prompt) as invoker:\n",
    "            hidden_states = []\n",
    "                    \n",
    "            for i in range(max_new_tokens):\n",
    "\n",
    "                for l in range(model.config.n_layer):\n",
    "                    if l in layers:\n",
    "                        if l == 0:\n",
    "                            raise ValueError(\"Cannot erase layer 0\")\n",
    "                        else:\n",
    "                            model.transformer.h[l].output = last_saved\n",
    "                    else:\n",
    "                        last_saved = model.transformer.h[l].output.save()\n",
    "                final_hs = model.lm_head.output\n",
    "                hidden_states.append(final_hs.save())\n",
    "                invoker.next()\n",
    "             \n",
    "            hidden_states.append(final_hs.save())\n",
    "            \n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "hidden_states = get_final_states(prompt, 4)\n",
    "hidden_states_erase = get_final_states_erase(prompt, 4, layers=list(range(2,24)))\n",
    "\n",
    "out = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states]\n",
    "out_erase = [util.apply(i, lambda x: x.value, Proxy) for i in hidden_states_erase]\n",
    "\n",
    "logit_diffs_q = []\n",
    "logit_diffs_t = []\n",
    "\n",
    "q_idx = model.tokenizer.encode(\"q\")[0]\n",
    "t_idx = model.tokenizer.encode(\"t\")[0]\n",
    "\n",
    "topk=10\n",
    "predictions = []\n",
    "for j in range(len(out)):\n",
    "    hs = out[j]\n",
    "    hs_erase = out_erase[j]\n",
    "    if j == 0:\n",
    "        hs = hs[:, -1,:]\n",
    "        hs = hs.unsqueeze(1)\n",
    "        hs_erase = hs_erase[:, -1,:]\n",
    "        hs_erase = hs_erase.unsqueeze(1)\n",
    "        \n",
    "    logits_clean = hs.softmax(dim=-1)\n",
    "    logits_erase = hs_erase.softmax(dim=-1)\n",
    "    \n",
    "    logit_clean_q = logits_clean[:,:,q_idx].item()\n",
    "    logit_erase_q = logits_erase[:,:,q_idx].item()\n",
    "    logit_clean_t = logits_clean[:,:,t_idx].item()\n",
    "    logit_erase_t = logits_erase[:,:,t_idx].item()\n",
    "    \n",
    "    max_logit_clean = logits_clean.argmax().item()\n",
    "    max_logit_erase = logits_erase.argmax().item()\n",
    "    \n",
    "    logit_diffs_q.append([logit_clean_q - logit_erase_q, logit_clean_q, logit_erase_q])\n",
    "    logit_diffs_t.append([logit_clean_t - logit_erase_t, logit_clean_t, logit_erase_t])\n",
    "    \n",
    "erase_prediction = model.tokenizer.decode(max_logit_erase)\n",
    "clean_prediction = model.tokenizer.decode(max_logit_clean)\n",
    "print(f\"Clean: {clean_prediction}, Erase: {erase_prediction}\")\n",
    "\n",
    "result_table = pd.DataFrame([logit_diffs_q[-1], logit_diffs_t[-1]], index=[\"q\",\"t\"], columns=[\"diff\", \"clean\", \"erase\"])\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
