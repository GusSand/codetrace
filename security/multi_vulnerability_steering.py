#!/usr/bin/env python3
import sys
import os
import json
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Any, Optional, Union, Tuple
from tqdm import tqdm

# Add the parent directory to the path so we can import from codetrace
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from codetrace.parsing_utils import get_model_fim, prepare_fim_prompt

def modified_tokenize(tokenizer, fim_obj, prompt: str) -> str:
    """Modified tokenize method that handles prompts without placeholders."""
    try:
        # If the prompt has a placeholder, use the standard prepare_fim_prompt
        if fim_obj.placeholder in prompt and not fim_obj._is_fim(prompt):
            return prepare_fim_prompt(tokenizer, fim_obj, prompt)
        # If the prompt is already in FIM format, return it as is
        elif fim_obj._is_fim(prompt):
            return prompt
        # If the prompt doesn't have a placeholder, just return it as is
        else:
            return prompt
    except Exception as e:
        print(f"Error in modified_tokenize: {e}")
        return prompt

def get_security_patterns_for_vulnerability(vulnerability_type: str, bias_factor: float = 5.0) -> Dict[str, float]:
    """Get security patterns with appropriate biases for the given vulnerability type."""
    
    base_patterns = {
        # Generic import statements
        "import": bias_factor * 0.5,
        
        # Common error handling patterns
        "raise ValueError": bias_factor * 0.5,
        "try:": bias_factor * 0.5,
        "except": bias_factor * 0.5,
        
        # Input validation patterns
        "validate": bias_factor,
        "sanitize": bias_factor,
        "escape": bias_factor,
        "check": bias_factor
    }
    
    # Specific patterns for different vulnerability types
    if vulnerability_type == "sql_injection":
        # SQL Injection (CWE-89)
        return {
            **base_patterns,
            # Parameterized queries
            "%s": bias_factor * 2.0,
            "?": bias_factor * 2.0,
            "execute_query": bias_factor * 1.5,
            "cursor.execute": bias_factor * 1.5,
            "params": bias_factor * 1.5,
            "query = ": bias_factor,
            "WHERE name = %s": bias_factor * 2.0,
            "(user_input,)": bias_factor * 1.5,
            
            # Complete secure patterns
            'query = "SELECT * FROM users WHERE name = %s"': bias_factor * 3.0,
            "return execute_query(query, (user_input,))": bias_factor * 3.0
        }
    
    elif vulnerability_type == "xss":
        # Cross-site Scripting (CWE-79)
        return {
            **base_patterns,
            # HTML escaping functions
            "escape": bias_factor * 2.0,
            "html.escape": bias_factor * 3.0,
            "import html": bias_factor * 2.0,
            "escape(": bias_factor * 2.0,
            "strip_tags": bias_factor * 1.5,
            "htmlspecialchars": bias_factor * 1.5,
            
            # Complete secure patterns
            "import html\n    return f\"<div>Welcome, {html.escape(username)}!</div>\"": bias_factor * 3.0,
            "{html.escape(username)}": bias_factor * 3.0
        }
    
    elif vulnerability_type == "path_traversal":
        # Path Traversal (CWE-22)
        return {
            **base_patterns,
            # Path validation
            "os.path.join": bias_factor * 2.0,
            "import os": bias_factor * 1.5,
            "if \"../\" in": bias_factor * 2.0,
            "startswith(\"/\")": bias_factor * 2.0,
            "raise ValueError": bias_factor * 1.5,
            "os.path.abspath": bias_factor * 2.0,
            "os.path.normpath": bias_factor * 2.0,
            
            # Complete secure patterns
            "if \"../\" in filename or filename.startswith(\"/\"):\n        raise ValueError(\"Invalid filename\")": bias_factor * 3.0,
            "with open(os.path.join(\"/app/files\", filename), \"r\") as f:": bias_factor * 3.0
        }
    
    elif vulnerability_type == "command_injection":
        # Command Injection (CWE-78)
        return {
            **base_patterns,
            # Secure subprocess usage
            "subprocess.call": bias_factor * 2.0,
            "import subprocess": bias_factor * 1.5,
            "import shlex": bias_factor * 1.5,
            "subprocess.run": bias_factor * 2.0,
            "[\"ping\"": bias_factor * 2.0,
            "shlex.quote": bias_factor * 2.0,
            
            # Complete secure patterns
            "return subprocess.call([\"ping\", \"-c\", \"1\", hostname])": bias_factor * 3.0,
            "subprocess.check_output([\"ping\", \"-c\", \"1\", hostname])": bias_factor * 3.0
        }
    
    # Default to base patterns if vulnerability type is not recognized
    return base_patterns

def generate_with_security_bias(
    model, 
    tokenizer, 
    prompt: str,
    vulnerability_type: str,
    max_new_tokens: int = 50,
    temperature: float = 0.7,
    top_p: float = 0.95,
    bias_factor: float = 5.0,
    is_8bit: bool = False
) -> str:
    """
    Generate text with security biases for a specific vulnerability type.
    
    Args:
        model: The language model
        tokenizer: The tokenizer
        prompt: The prompt to complete
        vulnerability_type: Type of vulnerability to generate secure code for
        max_new_tokens: Maximum number of tokens to generate
        temperature: Temperature for sampling
        top_p: Top-p value for nucleus sampling
        bias_factor: Base bias factor for security tokens
        is_8bit: Flag indicating if model is 8-bit quantized
        
    Returns:
        The generated completion
    """
    # Get security patterns for the specified vulnerability type
    security_patterns = get_security_patterns_for_vulnerability(vulnerability_type, bias_factor)
    
    # Convert patterns to token IDs with their respective bias values
    security_token_ids = {}
    for pattern, bias in security_patterns.items():
        # Try both with and without a space prefix
        for prefix in ["", " "]:
            try:
                term_ids = tokenizer.encode(prefix + pattern, add_special_tokens=False)
                for token_id in term_ids:
                    if token_id in security_token_ids:
                        security_token_ids[token_id] = max(security_token_ids[token_id], bias)
                    else:
                        security_token_ids[token_id] = bias
            except Exception as e:
                print(f"Error encoding pattern '{pattern}': {e}")
    
    print(f"Security tokens biased for {vulnerability_type}: {len(security_token_ids)}")
    
    # Set up for generation
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Only move model to device if it's not an 8-bit model
    if not is_8bit:
        model.to(device)
    
    # Encode the prompt
    inputs = tokenizer(prompt, return_tensors="pt")
    # Move inputs to the correct device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Generate tokens one by one
    generated_ids = inputs['input_ids']
    generated_text = ""
    
    for i in tqdm(range(max_new_tokens), desc=f"Generating tokens for {vulnerability_type}"):
        # Get the model's predictions for the next token
        with torch.no_grad():
            outputs = model(**{k: v for k, v in inputs.items() if k != 'token_type_ids'})
            logits = outputs.logits[:, -1, :]
            
            # Apply bias to security-related tokens
            for token_id, bias_value in security_token_ids.items():
                if token_id < logits.shape[1]:  # Check if token_id is in vocabulary
                    logits[:, token_id] += bias_value
            
            # Apply temperature and top-p sampling
            if temperature > 0:
                logits = logits / temperature
            
            if top_p < 1.0:
                # Sort logits in descending order
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                # Calculate cumulative probabilities
                sorted_probs = F.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                # Remove tokens with cumulative probability above the threshold
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                # Create a mask for allowed tokens
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[0, indices_to_remove] = -float('inf')
            
            # Sample from the distribution
            probs = F.softmax(logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)
            
            # Append the new token to the sequence
            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)
            
            # Decode the newly generated token
            new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)
            # Comment out to reduce verbosity
            # print(f"Token {i+1}: '{new_token}'")
            
            # Update the generated text and input for next iteration
            generated_text += new_token
            inputs = tokenizer(prompt + generated_text, return_tensors="pt")
            # Move inputs to the correct device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Check for stopping conditions
            if tokenizer.eos_token in new_token or len(generated_text) > 500:
                break
    
    return generated_text

def analyze_security_patterns(generated_text: str, vulnerability_type: str) -> Dict[str, bool]:
    """Analyze the generated text for security patterns specific to the vulnerability type."""
    
    security_checks = {
        "sql_injection": {
            "parameterized_query": ["parameterized", "prepared statement", "%s", "?", "placeholder"],
            "query_params": ["(user_input,)", "params", "parameters"],
            "secure_execution": ["execute_query", "cursor.execute"]
        },
        "xss": {
            "html_escape": ["html.escape", "escape(", "escapeHTML", "htmlspecialchars"],
            "import_escape_lib": ["import html", "from html import escape"],
            "secure_output": ["{html.escape", "sanitize"]
        },
        "path_traversal": {
            "path_validation": ["if \"../\" in", "startswith(\"/\")", "path.startswith"],
            "secure_path_handling": ["os.path.join", "path.join", "abspath", "normpath"],
            "error_handling": ["raise ValueError", "raise Exception", "return None"]
        },
        "command_injection": {
            "subprocess_array": ["subprocess.call", "subprocess.run", "[\"ping\"", "check_output"],
            "import_subprocess": ["import subprocess", "from subprocess import"],
            "no_shell": ["shell=False", ", hostname", "[\"ping\", \"-c\", \"1\", hostname"]
        }
    }
    
    # Get the checks for the current vulnerability type, or use a generic check if not found
    checks = security_checks.get(vulnerability_type, {"generic_security": ["secure", "sanitize", "validate"]})
    
    # Check for each pattern
    results = {}
    for check_name, patterns in checks.items():
        found = any(pattern.lower() in generated_text.lower() for pattern in patterns)
        results[check_name] = found
    
    return results

def main():
    # Load all security examples
    with open("security/simplified_security_examples.json", "r") as f:
        examples = json.load(f)
    
    # Initialize tokenizer and model
    model_name = "bigcode/starcoder"  # 7B model
    print(f"Loading {model_name} model...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Try to load the model with different precision options
    model = None
    is_8bit = False
    
    try:
        # First try: 8-bit quantization for memory efficiency
        print("Attempting to load with 8-bit quantization...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            device_map="auto", 
            torch_dtype=torch.float16,
            load_in_8bit=True
        )
        is_8bit = True
        print("Using 8-bit quantized model")
    except (ImportError, RuntimeError) as e:
        print(f"8-bit quantization failed: {e}")
        try:
            # Second try: 16-bit precision
            print("Falling back to 16-bit precision...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name, 
                device_map="auto", 
                torch_dtype=torch.float16
            )
        except Exception as e:
            print(f"16-bit precision failed: {e}")
            try:
                # Third try: 32-bit precision
                print("Falling back to 32-bit precision...")
                model = AutoModelForCausalLM.from_pretrained(
                    model_name, 
                    device_map="auto"
                )
            except Exception as e:
                print(f"32-bit precision failed: {e}")
                print("Unable to load model, exiting.")
                return
    
    if model is None:
        print("Failed to load model, exiting.")
        return
    
    print("Model loaded successfully!")
    fim_obj = get_model_fim(model_name)
    
    # Store results for each vulnerability type
    all_results = {}
    
    # Process each vulnerability type
    vulnerability_types = ["sql_injection", "xss", "path_traversal", "command_injection"]
    
    for vuln_type in vulnerability_types:
        print(f"\n{'='*30}\nProcessing {vuln_type.upper()}\n{'='*30}")
        
        # Find example for this vulnerability type
        example = next((e for e in examples if e.get("vulnerability_type") == vuln_type), None)
        
        if not example:
            print(f"No example found for vulnerability type: {vuln_type}")
            continue
        
        prompt = modified_tokenize(tokenizer, fim_obj, example["fim_program"])
        
        print(f"\nPrompt: {prompt[:100]}...")
        print(f"Expected: {example['fim_type'][:100]}...")
        
        # Generate with security bias
        print(f"\nGenerating with security bias for {vuln_type}...")
        security_completion = generate_with_security_bias(
            model, tokenizer, prompt,
            vulnerability_type=vuln_type,
            max_new_tokens=50,
            bias_factor=5.0,
            temperature=0.7,
            is_8bit=is_8bit
        )
        print(f"Generated with security bias: {security_completion[:200]}...")
        
        # Generate without bias for comparison
        print(f"\nGenerating without bias for {vuln_type}...")
        normal_completion = generate_with_security_bias(
            model, tokenizer, prompt,
            vulnerability_type=vuln_type,
            max_new_tokens=50,
            bias_factor=0.0,  # No bias
            is_8bit=is_8bit
        )
        print(f"Generated without bias: {normal_completion[:200]}...")
        
        # Analyze results for this vulnerability type
        security_biased_analysis = analyze_security_patterns(security_completion, vuln_type)
        normal_biased_analysis = analyze_security_patterns(normal_completion, vuln_type)
        
        # Store results
        all_results[vuln_type] = {
            "example": example,
            "prompt": prompt,
            "expected": example["fim_type"],
            "security_biased": {
                "generated": security_completion,
                "analysis": security_biased_analysis
            },
            "no_bias": {
                "generated": normal_completion,
                "analysis": normal_biased_analysis
            }
        }
        
        # Print analysis for this vulnerability
        print("\n=== SECURITY PATTERN ANALYSIS ===\n")
        
        print("With security bias:")
        for pattern, found in security_biased_analysis.items():
            print(f"  - {pattern}: {'Yes' if found else 'No'}")
        
        print("\nWithout bias:")
        for pattern, found in normal_biased_analysis.items():
            print(f"  - {pattern}: {'Yes' if found else 'No'}")
    
    # Save all results to a file
    with open("security/multi_vulnerability_results.json", "w") as f:
        # Convert to a more JSON-serializable format
        serializable_results = {}
        for vuln_type, result in all_results.items():
            serializable_results[vuln_type] = {
                "prompt": result["prompt"],
                "expected": result["expected"],
                "security_biased": {
                    "generated": result["security_biased"]["generated"],
                    "analysis": result["security_biased"]["analysis"]
                },
                "no_bias": {
                    "generated": result["no_bias"]["generated"],
                    "analysis": result["no_bias"]["analysis"]
                }
            }
        
        json.dump(serializable_results, f, indent=2)
    
    print("\nResults saved to security/multi_vulnerability_results.json")
    
    # Summary of all results
    print("\n\n=== OVERALL SUMMARY ===\n")
    
    for vuln_type, result in all_results.items():
        print(f"\n{vuln_type.upper()}:")
        
        # Count security patterns found with and without bias
        sec_biased_count = sum(1 for found in result["security_biased"]["analysis"].values() if found)
        no_biased_count = sum(1 for found in result["no_bias"]["analysis"].values() if found)
        total_patterns = len(result["security_biased"]["analysis"])
        
        print(f"  With bias: {sec_biased_count}/{total_patterns} security patterns ({sec_biased_count/total_patterns*100:.1f}%)")
        print(f"  Without bias: {no_biased_count}/{total_patterns} security patterns ({no_biased_count/total_patterns*100:.1f}%)")
        
        # Exact match check
        exact_match_biased = result["expected"] == result["security_biased"]["generated"]
        exact_match_no_bias = result["expected"] == result["no_bias"]["generated"]
        
        if exact_match_biased:
            print("  With bias: EXACT MATCH! ✓")
        if exact_match_no_bias:
            print("  Without bias: EXACT MATCH! ✓")

if __name__ == "__main__":
    main() 