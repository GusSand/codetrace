{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d3d45-da94-40bc-973e-224f2ec65ac1",
   "metadata": {},
   "source": [
    "## Test type-inf dataset on LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67123b58-41f0-4771-91c0-e7e5a6b7a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# # set cuda visible device\n",
    "!export CUDA_VISIBLE_DEVICES=3\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "    \n",
    "# !export TRANSFORMERS_CACHE=/work/arjunguha-research-group/franlucc\n",
    "# root = \"/work/arjunguha-research-group\"\n",
    "\n",
    "root = \"/home/arjun/models/\"\n",
    "# codellama7b = f\"{root}/models/CodeLlama-7b-hf\"\n",
    "# codellama13b = f\"{root}/models/CodeLlama-13b-hf\"\n",
    "# starcoder = f\"{root}/arjun/models/starcoderbase\"\n",
    "starcoder7b = f\"{root}/starcoderbase-7b\"\n",
    "starcoder1b = f\"{root}/starcoderbase-1b\"\n",
    "\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "from build_dataset import *\n",
    "\n",
    "# fim special toks\n",
    "# codellama is finnicky with these spaces: \"<PRE> {prefix} <SUF>{suffix} <MID>\" or \"<PRE> {prefix} <SUF> {suffix} <MID>\"\n",
    "codellama_fim = FimObj(\"<PRE> \", \" <SUF> \",\" <MID>\", \"<FILL>\")\n",
    "starcoder_fim = FimObj(\"<fim_prefix>\", \"<fim_suffix>\",\"<fim_middle>\", \"<FILL>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892562ff-2cff-4331-8a3b-a3f1978035fa",
   "metadata": {},
   "source": [
    "## REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1fa37a3-9552-46af-a8e9-114bef08b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [00:02<00:00, 167.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 20:20:40 llm_engine.py:72] Initializing an LLM engine with config: model='/home/arjun/models//starcoderbase-7b', tokenizer='/home/arjun/models//starcoderbase-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 20:21:04 llm_engine.py:207] # GPU blocks: 176864, # CPU blocks: 12483\n"
     ]
    }
   ],
   "source": [
    "from evaluate import *\n",
    "import datasets\n",
    "ds = datasets.load_dataset(\"franlucc/stenotype-eval-dataset\", split=\"train\")\n",
    "fim_examples = fim_dataset(ds)\n",
    "\n",
    "llm = LLM(model=starcoder7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3e943-e412-4df5-9782-fac25abf0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import *\n",
    "import subprocess\n",
    "import tempfile\n",
    "params = SamplingParams(temperature=0, max_tokens=10)\n",
    "prog = open(\"ts-benchmark/curried_renamed_no_examples/tree_postorder.ts\").read()\n",
    "prompts = [\n",
    "  fim_prog(prog)[-2], # some FIM'd variation\n",
    "]\n",
    "\n",
    "for k,p in enumerate(prompts):\n",
    "  p = placeholder_to_std_fmt(p, starcoder_fim)\n",
    "  out = llm.generate([p], params)\n",
    "  generated_text = out[0].outputs[0].text\n",
    "  \n",
    "  # # find index of <fim_suffix> in prompt\n",
    "  # suffix_idx = p.index(\"<fim_suffix>\")\n",
    "  # # display index as (row, column)\n",
    "  # suffix_idx = (p[:suffix_idx].count(\"\\n\"), suffix_idx - p[:suffix_idx].rfind(\"\\n\"))\n",
    "  # print(f\"suffix_idx: {suffix_idx}\")\n",
    "  # print(f\"PROG:\\n{p!r}\")\n",
    "  # print(f\"GEN:\\n{generated_text}\")\n",
    "  \n",
    "  # write to file temp.ts\n",
    "  with open(\"temp.ts\", \"w\") as f:\n",
    "    f.write(unfim(p+generated_text, starcoder_fim))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ab8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "becc8c66-2f84-447f-9d2f-5b933ef8cd7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98eb32-9406-4abe-9a50-897ee6893f7d",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f01e838-9996-41bb-94e2-6245a22e6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 15:45:38 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-18 15:45:43 llm_engine.py:275] # GPU blocks: 7226, # CPU blocks: 512\n",
      "INFO 01-18 15:45:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 15:45:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 15:45:47 model_runner.py:547] Graph capturing finished in 2 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=codellama7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790c075-6eaa-473a-89f6-d1d665f059c2",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c608f-09c1-4dd8-99fd-d49a2bbd4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:01<00:00, 10.51it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.56it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts()\n",
    "generate_test(prompts, llm, \"codellama7b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0b2d9-eaaf-411b-81f1-5543fa17d8b3",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d525af-0d4d-4b8e-bc2e-59afecc6c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:02<00:00,  7.49it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  2.59it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:06<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama7b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a97d0-a80b-499e-b6d9-dfcf22ee5061",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa0a009-893f-482a-969f-c950620080ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_45_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"BookTitle | unknown\"\n",
      "        ],\n",
      "        \"BookTitle\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_46_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"unknown\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bookmark_fim_null_greedy_47_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"null\"\n",
      "        ],\n",
      "        \"null\"\n",
      "    ],\n",
      "    \"date_fim_DateQuery_greedy_48_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"DateQuery {\\n\\tif (readableDate=='') return empty()\\n\\tif (readableDate.match(/^(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)T(\\\\d\\\\\"\n",
      "        ],\n",
      "        \"DateQuery\"\n",
      "    ],\n",
      "    \"date_fim_ValidDateQuery_greedy_49_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"ValidDateQuery\"\n",
      "        ],\n",
      "        \"ValidDateQuery\"\n",
      "    ],\n",
      "    \"election_fim_T[]_greedy_50_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"T[]): boolean {\\n  const sortedA = sortedArrayOfAnyElectionObjects(a);\\n  const sortedB = sortedArrayOfAnyElectionObjects(b);\\n  if (sortedA.length !== sortedB.length\"\n",
      "        ],\n",
      "        \"T[]\"\n",
      "    ],\n",
      "    \"grype_fim_[]_greedy_51_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"[]\"\n",
      "        ],\n",
      "        \"[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_52_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"any[]\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"magic_fim_MagicMethodResult_greedy_53_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"MagicMethodResult\\n      \"\n",
      "        ],\n",
      "        \"MagicMethodResult\"\n",
      "    ],\n",
      "    \"magic_fim_PipeObject_greedy_54_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeObject\"\n",
      "        ],\n",
      "        \"PipeObject\"\n",
      "    ],\n",
      "    \"magic_fim_PipeParam_greedy_55_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeParam\"\n",
      "        ],\n",
      "        \"PipeParam\"\n",
      "    ],\n",
      "    \"svm_fim_SvmSymbol[]_greedy_56_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"SvmSymbol[]\"\n",
      "        ],\n",
      "        \"SvmSymbol[]\"\n",
      "    ],\n",
      "    \"transaction_fim_any_greedy_57_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"any = {},\\n        credentials: any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_58_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<any> {\\n        throw Error(\\n            `NotImplementedError: ${this.name}.initialize() has not been implemented.`\\n        );\\n    }\\n\\n    /**\\n     * Fetch balances from\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ],\n",
      "    \"tree_fim_Node__greedy_59_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Node_Type\"\n",
      "        ],\n",
      "        \"Node\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_12_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"BookTitle\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_13_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_3_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bookmark_fim_null_greedy_3_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"null\"\n",
      "        ],\n",
      "        \"null\"\n",
      "    ],\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"DateQuery\"\n",
      "        ],\n",
      "        \"DateQuery\"\n",
      "    ],\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"ValidDateQuery\"\n",
      "        ],\n",
      "        \"ValidDateQuery\"\n",
      "    ],\n",
      "    \"election_fim_T[]_greedy_15_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"T[]\"\n",
      "        ],\n",
      "        \"T[]\"\n",
      "    ],\n",
      "    \"grype_fim_[]_greedy_6_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_15_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_7_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"magic_fim_MagicMethodResult_greedy_15_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"MagicMethodResult\"\n",
      "        ],\n",
      "        \"MagicMethodResult\"\n",
      "    ],\n",
      "    \"magic_fim_PipeObject_greedy_16_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeObject\"\n",
      "        ],\n",
      "        \"PipeObject\"\n",
      "    ],\n",
      "    \"magic_fim_PipeParam_greedy_17_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeParam\"\n",
      "        ],\n",
      "        \"PipeParam\"\n",
      "    ],\n",
      "    \"svm_fim_SvmSymbol[]_greedy_9_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  return {\\n    type: 'array',\\n    itemType: parseSvmType(type),\\n    size: parseInt(length, 10),\\n    originalType: rawType,\\n  };\\n}\\n\"\n",
      "        ],\n",
      "        \"SvmSymbol[]\"\n",
      "    ],\n",
      "    \"transaction_fim_any_greedy_6_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_7_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<any> {\\n        throw Error(\\n            `NotImplementedError: ${this.name}.initialize() has not been implemented.`\\n        );\\n    }\\n\\n    /**\\n     * Fetch balances from\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ],\n",
      "    \"tree_fim_Node__greedy_3_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n\\t<MID>\\n\\t\\t<MID>\\n\\t\\t\\t<MID>\\n\\t\\t\\t\\t<MID>\\n\\t\\t\\t\\t\\t<MID>\\n\\t\\t\\t\\t\\t\\t<MID\"\n",
      "        ],\n",
      "        \"Node\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "def pretty_print(dict, targets = None):\n",
    "    if targets:\n",
    "        dict = {k:v for k,v in dict.items() if all([t in k for t in targets])}\n",
    "    print(json.dumps(dict, indent=4))\n",
    "\n",
    "st = type_inf_eval(\"generations/codellama7b/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/codellama7b/multitok\", verbose)\n",
    "targ = [\"greedy\"]\n",
    "# pretty_print(st, targ)\n",
    "pretty_print(mt, targ)\n",
    "\n",
    "st = type_inf_eval(\"generations/v0_codellama7b/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/v0_codellama7b/multitok\", verbose)\n",
    "targ = [\"greedy\"]\n",
    "# pretty_print(st, targ)\n",
    "pretty_print(mt, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64942bf8-524a-4645-9e5e-6c66bdc873f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FIM sanity check\n",
    "since Codellama is getting all 0s, sanity check with a FIM example from CodeLlama tut: \n",
    "[codellama infill example](https://github.com/facebookresearch/codellama/blob/main/example_infilling.py)\n",
    "\n",
    "**EDIT** the problem was CodeLlama wants spaces in its special fim tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad664d2-7287-43a0-a250-da2f662475e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PRE> def remove_non_ascii(s: str) -> str:\\n    \"\"\"  <SUF>\\n    return result <MID>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove non-ASCII characters from a string.\n",
      "\n",
      "    Args:\n",
      "        s (str): The string to remove non-ASCII characters from.\n",
      "\n",
      "    Returns:\n",
      "        str: The string with non-ASCII characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cont = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result'''\n",
    "\n",
    "prompts = [placeholder_to_std_fmt(cont, codellama_fim)]\n",
    "print(prompts)\n",
    "output = llm.generate(prompts, greedy_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d57c5-c59a-4e1b-9ee4-d8750bbb0738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 13b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79662f93-64c0-451c-bf54-b5cfbe89e33e",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc366e-466c-42ed-8784-58bdb85f164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 20:31:53 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-17 20:32:01 llm_engine.py:275] # GPU blocks: 3631, # CPU blocks: 327\n",
      "INFO 01-17 20:32:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-17 20:32:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-17 20:32:06 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=codellama13b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c80a0-ac22-49ab-8083-7b31f3f6b8f9",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8205405-c60f-467f-b8f0-d88bac7bbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts(targets=[\"bible\"])\n",
    "generate_test(prompts, llm, \"codellama13b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488dfb-ef22-4378-8e97-e88f165732f8",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5fcfb9-f924-4400-b509-06a8144a8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama13b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e532c-b242-417c-9081-a9ef2675f6a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033ef5c2-23b9-490e-b6a5-b0c7d90ca7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_0_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_2_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_1_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_3_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_5_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_0_n1\": false,\n",
      "    \"date_fim_DateQuery_temp_0.2_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_greedy_1_n1\": false,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_3_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_5_n20\": 0.0\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_6_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_8_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_10_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_7_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_9_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_11_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": true,\n",
      "    \"date_fim_DateQuery_temp_0.2_8_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_10_n20\": 0.85,\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": true,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_9_n20\": 1.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_11_n20\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/singletok\", verbose), indent=4))\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/multitok\", verbose), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15650acb-eb97-4b45-8e85-29d39a59593c",
   "metadata": {},
   "source": [
    "## StarcoderBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4bc6fc-d048-446e-8149-5f9483a5d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 13:44:40 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-18 13:45:20 llm_engine.py:275] # GPU blocks: 131427, # CPU blocks: 13107\n",
      "INFO 01-18 13:45:22 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 13:45:22 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 13:45:25 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=starcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d474f-21a6-4fae-a3ae-55193d35edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts()\n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              1, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) \n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              20, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c02dd-6436-4000-ad8b-18f2da2a808b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f17cc41-0197-4f52-9059-859ba1ce2c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"transaction_fim_any_greedy_54_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_55_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"transaction_fim_any_greedy_60_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_61_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<void>\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "def pretty_print(dict, targets = None):\n",
    "    if targets:\n",
    "        dict = {k:v for k,v in dict.items() if all([t in k for t in targets])}\n",
    "    print(json.dumps(dict, indent=4))\n",
    "\n",
    "st = type_inf_eval(\"generations/starcoder/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/starcoder/multitok\", verbose)\n",
    "\n",
    "targ = [\"greedy\", \"transaction\"]\n",
    "pretty_print(st, targ)\n",
    "pretty_print(mt, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d15df0-fc5b-4066-ba4a-e87c27f82ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
