{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d3d45-da94-40bc-973e-224f2ec65ac1",
   "metadata": {},
   "source": [
    "## Test type-inf dataset on LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67123b58-41f0-4771-91c0-e7e5a6b7a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# # set cuda visible device\n",
    "# !export CUDA_VISIBLE_DEVICES=0\n",
    "    \n",
    "!export TRANSFORMERS_CACHE=/work/arjunguha-research-group/franlucc\n",
    "root = \"/work/arjunguha-research-group\"\n",
    "codellama7b = f\"{root}/models/CodeLlama-7b-hf\"\n",
    "codellama13b = f\"{root}/models/CodeLlama-13b-hf\"\n",
    "starcoder = f\"{root}/arjun/models/starcoderbase\"\n",
    "\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "from build_dataset import *\n",
    "\n",
    "# fim special toks\n",
    "# codellama is finnicky with these spaces: \"<PRE> {prefix} <SUF>{suffix} <MID>\" or \"<PRE> {prefix} <SUF> {suffix} <MID>\"\n",
    "codellama_fim = FimObj(\"<PRE> \", \" <SUF> \",\" <MID>\", \"<FILL>\")\n",
    "starcoder_fim = FimObj(\"<fim_prefix>\", \"<fim_suffix>\",\"<fim_middle>\", \"<FILL>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892562ff-2cff-4331-8a3b-a3f1978035fa",
   "metadata": {},
   "source": [
    "## REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1fa37a3-9552-46af-a8e9-114bef08b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [00:01<00:00, 321.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 21:09:47 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 21:10:39 llm_engine.py:275] # GPU blocks: 131427, # CPU blocks: 13107\n",
      "INFO 01-18 21:10:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 21:10:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 21:10:50 model_runner.py:547] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  45%|████▍     | 26/58 [00:06<00:07,  4.21it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fim_dataset\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/arjunguha-research-group/mhyee/datasets/stenotype-eval-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m fim_examples \u001b[38;5;241m=\u001b[39m fim_dataset(ds)\n",
      "File \u001b[0;32m/work/arjunguha-research-group/franlucc/projects/fim_interp/evaluate.py:124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,ex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fim_examples):\n\u001b[1;32m    123\u001b[0m   prompts \u001b[38;5;241m=\u001b[39m [placeholder_to_std_fmt(p, starcoder_fim) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m ex]\n\u001b[0;32m--> 124\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i,output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(out):\n\u001b[1;32m    126\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompts[i]\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/entrypoints/llm.py:165\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    163\u001b[0m         i]\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(prompt, sampling_params, token_ids)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/entrypoints/llm.py:185\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    183\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 185\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/engine/llm_engine.py:628\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/engine/llm_engine.py:795\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/worker/worker.py:189\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 189\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/worker/model_runner.py:461\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    453\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m model_executable(\n\u001b[1;32m    454\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_tokens,\n\u001b[1;32m    455\u001b[0m     positions\u001b[38;5;241m=\u001b[39minput_positions,\n\u001b[1;32m    456\u001b[0m     kv_caches\u001b[38;5;241m=\u001b[39mkv_caches,\n\u001b[1;32m    457\u001b[0m     input_metadata\u001b[38;5;241m=\u001b[39minput_metadata,\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/model_executor/models/gpt_bigcode.py:258\u001b[0m, in \u001b[0;36mGPTBigCodeForCausalLM.sample\u001b[0;34m(self, hidden_states, sampling_metadata)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    255\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    256\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    257\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 258\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/model_executor/layers/sampler.py:93\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, embedding, hidden_states, sampling_metadata, embedding_bias)\u001b[0m\n\u001b[1;32m     90\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     96\u001b[0m     logprobs, sampling_metadata, sample_results)\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/model_executor/layers/sampler.py:408\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    405\u001b[0m seq_group_ids, seq_groups, is_prompts, sample_indices \u001b[38;5;241m=\u001b[39m sample_metadata[\n\u001b[1;32m    406\u001b[0m     sampling_type]\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mGREEDY:\n\u001b[0;32m--> 408\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_greedy_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mRANDOM:\n\u001b[1;32m    410\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups, is_prompts,\n\u001b[1;32m    411\u001b[0m                                     multinomial_samples)\n",
      "File \u001b[0;32m~/.conda/envs/a100_causal_2/lib/python3.8/site-packages/vllm/model_executor/layers/sampler.py:241\u001b[0m, in \u001b[0;36m_greedy_sample\u001b[0;34m(selected_seq_groups, samples)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_greedy_sample\u001b[39m(\n\u001b[1;32m    238\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    239\u001b[0m     samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 241\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from evaluate import fim_dataset\n",
    "ds = datasets.load_from_disk(\"/work/arjunguha-research-group/mhyee/datasets/stenotype-eval-dataset\")\n",
    "fim_examples = fim_dataset(ds)\n",
    "\n",
    "llm = LLM(model=starcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3e943-e412-4df5-9782-fac25abf0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(temperature=0, max_tokens=1)\n",
    "\n",
    "for k,ex in enumerate(fim_examples[:2]):\n",
    "  prompts = [placeholder_to_std_fmt(p, starcoder_fim) for p in ex]\n",
    "  out = llm.generate(prompts, params)\n",
    "  for i,output in enumerate(out):\n",
    "    prompt = prompts[i]\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72bc3355-a991-4b4f-8a25-d30e5a69cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 19:11:25 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-18 19:11:59 llm_engine.py:275] # GPU blocks: 131427, # CPU blocks: 13107\n",
      "INFO 01-18 19:12:00 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 19:12:00 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 19:12:03 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=starcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d177da-9d2b-4f2d-8712-b5cb7fa561e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PRE>function F(x: <fim_suffix>) { return x + 1; }<fim_middle>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 38.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"number\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "'''\n",
    "interface Person {\n",
    "  name: string;\n",
    "  next: Line;\n",
    "}\n",
    "\n",
    "type Line = \"end of line\" | <FILL>;\n",
    "\n",
    "// Examples of Line\n",
    "const LINE_EX_1: Line = \"end of line\";\n",
    "const LINE_EX_2: Line = { name: \"Alice\", next: \"end of line\" };\n",
    "const LINE_EX_3: Line = { name: \"Alice\", next: { name: \"Bob\", next: \"end of line\" } };\n",
    "\n",
    "// Counts the number of people in the line.\n",
    "function countPeople(ln: Line): number {\n",
    "  if (typeof ln === \"string\" && ln === \"end of line\") {\n",
    "    return 0;\n",
    "  } else {\n",
    "    return 1 + countPeople(ln.next);\n",
    "  }\n",
    "}\n",
    "'''.strip(),\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    '''function F(x: <FILL>) { return x + 1; }'''\n",
    "]\n",
    "prompts = [placeholder_to_std_fmt(p, starcoder_fim) for p in prompts]\n",
    "print(f\"{prompts[0]!r}\")\n",
    "\n",
    "#greedy\n",
    "params = SamplingParams(temperature=0, max_tokens=1)\n",
    "out = llm.generate(prompts, params)\n",
    "\n",
    "for i,output in enumerate(out):\n",
    "    generated_text = []\n",
    "    for j in range(len(output.outputs)):\n",
    "        text = output.outputs[j].text\n",
    "        generated_text.append(text)\n",
    "    print(json.dumps(generated_text, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc8c66-2f84-447f-9d2f-5b933ef8cd7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98eb32-9406-4abe-9a50-897ee6893f7d",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f01e838-9996-41bb-94e2-6245a22e6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 15:45:38 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-18 15:45:43 llm_engine.py:275] # GPU blocks: 7226, # CPU blocks: 512\n",
      "INFO 01-18 15:45:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 15:45:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 15:45:47 model_runner.py:547] Graph capturing finished in 2 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=codellama7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790c075-6eaa-473a-89f6-d1d665f059c2",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c608f-09c1-4dd8-99fd-d49a2bbd4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:01<00:00, 10.51it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.56it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts()\n",
    "generate_test(prompts, llm, \"codellama7b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0b2d9-eaaf-411b-81f1-5543fa17d8b3",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d525af-0d4d-4b8e-bc2e-59afecc6c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:02<00:00,  7.49it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  2.59it/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:06<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama7b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a97d0-a80b-499e-b6d9-dfcf22ee5061",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa0a009-893f-482a-969f-c950620080ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_45_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"BookTitle | unknown\"\n",
      "        ],\n",
      "        \"BookTitle\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_46_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"unknown\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bookmark_fim_null_greedy_47_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"null\"\n",
      "        ],\n",
      "        \"null\"\n",
      "    ],\n",
      "    \"date_fim_DateQuery_greedy_48_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"DateQuery {\\n\\tif (readableDate=='') return empty()\\n\\tif (readableDate.match(/^(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)T(\\\\d\\\\\"\n",
      "        ],\n",
      "        \"DateQuery\"\n",
      "    ],\n",
      "    \"date_fim_ValidDateQuery_greedy_49_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"ValidDateQuery\"\n",
      "        ],\n",
      "        \"ValidDateQuery\"\n",
      "    ],\n",
      "    \"election_fim_T[]_greedy_50_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"T[]): boolean {\\n  const sortedA = sortedArrayOfAnyElectionObjects(a);\\n  const sortedB = sortedArrayOfAnyElectionObjects(b);\\n  if (sortedA.length !== sortedB.length\"\n",
      "        ],\n",
      "        \"T[]\"\n",
      "    ],\n",
      "    \"grype_fim_[]_greedy_51_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"[]\"\n",
      "        ],\n",
      "        \"[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_52_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"any[]\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"magic_fim_MagicMethodResult_greedy_53_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"MagicMethodResult\\n      \"\n",
      "        ],\n",
      "        \"MagicMethodResult\"\n",
      "    ],\n",
      "    \"magic_fim_PipeObject_greedy_54_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeObject\"\n",
      "        ],\n",
      "        \"PipeObject\"\n",
      "    ],\n",
      "    \"magic_fim_PipeParam_greedy_55_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeParam\"\n",
      "        ],\n",
      "        \"PipeParam\"\n",
      "    ],\n",
      "    \"svm_fim_SvmSymbol[]_greedy_56_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"SvmSymbol[]\"\n",
      "        ],\n",
      "        \"SvmSymbol[]\"\n",
      "    ],\n",
      "    \"transaction_fim_any_greedy_57_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"any = {},\\n        credentials: any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_58_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<any> {\\n        throw Error(\\n            `NotImplementedError: ${this.name}.initialize() has not been implemented.`\\n        );\\n    }\\n\\n    /**\\n     * Fetch balances from\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ],\n",
      "    \"tree_fim_Node__greedy_59_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Node_Type\"\n",
      "        ],\n",
      "        \"Node\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_12_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"BookTitle\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_13_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bible_fim_unknown_greedy_3_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  '2 Kings',\\n  '1 Chronicles',\\n  '2 Chronicles',\\n  Ezra,\\n  Nehemiah,\\n  Esther,\\n  Job,\\n  Psalms,\\n  Prover\"\n",
      "        ],\n",
      "        \"unknown\"\n",
      "    ],\n",
      "    \"bookmark_fim_null_greedy_3_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"null\"\n",
      "        ],\n",
      "        \"null\"\n",
      "    ],\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"DateQuery\"\n",
      "        ],\n",
      "        \"DateQuery\"\n",
      "    ],\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"ValidDateQuery\"\n",
      "        ],\n",
      "        \"ValidDateQuery\"\n",
      "    ],\n",
      "    \"election_fim_T[]_greedy_15_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"T[]\"\n",
      "        ],\n",
      "        \"T[]\"\n",
      "    ],\n",
      "    \"grype_fim_[]_greedy_6_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_15_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"grype_fim_string[]_greedy_7_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n *\\n * Licensed under the Business Source License v1.1\\n * (the \\\"License\\\"); you may not use this file except in compliance with the\\n * License. You may obtain a copy of the License at\\n\"\n",
      "        ],\n",
      "        \"string[]\"\n",
      "    ],\n",
      "    \"magic_fim_MagicMethodResult_greedy_15_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"MagicMethodResult\"\n",
      "        ],\n",
      "        \"MagicMethodResult\"\n",
      "    ],\n",
      "    \"magic_fim_PipeObject_greedy_16_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeObject\"\n",
      "        ],\n",
      "        \"PipeObject\"\n",
      "    ],\n",
      "    \"magic_fim_PipeParam_greedy_17_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"PipeParam\"\n",
      "        ],\n",
      "        \"PipeParam\"\n",
      "    ],\n",
      "    \"svm_fim_SvmSymbol[]_greedy_9_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n  return {\\n    type: 'array',\\n    itemType: parseSvmType(type),\\n    size: parseInt(length, 10),\\n    originalType: rawType,\\n  };\\n}\\n\"\n",
      "        ],\n",
      "        \"SvmSymbol[]\"\n",
      "    ],\n",
      "    \"transaction_fim_any_greedy_6_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_7_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<any> {\\n        throw Error(\\n            `NotImplementedError: ${this.name}.initialize() has not been implemented.`\\n        );\\n    }\\n\\n    /**\\n     * Fetch balances from\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ],\n",
      "    \"tree_fim_Node__greedy_3_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"\\n\\t<MID>\\n\\t\\t<MID>\\n\\t\\t\\t<MID>\\n\\t\\t\\t\\t<MID>\\n\\t\\t\\t\\t\\t<MID>\\n\\t\\t\\t\\t\\t\\t<MID\"\n",
      "        ],\n",
      "        \"Node\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "def pretty_print(dict, targets = None):\n",
    "    if targets:\n",
    "        dict = {k:v for k,v in dict.items() if all([t in k for t in targets])}\n",
    "    print(json.dumps(dict, indent=4))\n",
    "\n",
    "st = type_inf_eval(\"generations/codellama7b/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/codellama7b/multitok\", verbose)\n",
    "targ = [\"greedy\"]\n",
    "# pretty_print(st, targ)\n",
    "pretty_print(mt, targ)\n",
    "\n",
    "st = type_inf_eval(\"generations/v0_codellama7b/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/v0_codellama7b/multitok\", verbose)\n",
    "targ = [\"greedy\"]\n",
    "# pretty_print(st, targ)\n",
    "pretty_print(mt, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64942bf8-524a-4645-9e5e-6c66bdc873f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FIM sanity check\n",
    "since Codellama is getting all 0s, sanity check with a FIM example from CodeLlama tut: \n",
    "[codellama infill example](https://github.com/facebookresearch/codellama/blob/main/example_infilling.py)\n",
    "\n",
    "**EDIT** the problem was CodeLlama wants spaces in its special fim tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad664d2-7287-43a0-a250-da2f662475e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PRE> def remove_non_ascii(s: str) -> str:\\n    \"\"\"  <SUF>\\n    return result <MID>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove non-ASCII characters from a string.\n",
      "\n",
      "    Args:\n",
      "        s (str): The string to remove non-ASCII characters from.\n",
      "\n",
      "    Returns:\n",
      "        str: The string with non-ASCII characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cont = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result'''\n",
    "\n",
    "prompts = [placeholder_to_std_fmt(cont, codellama_fim)]\n",
    "print(prompts)\n",
    "output = llm.generate(prompts, greedy_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d57c5-c59a-4e1b-9ee4-d8750bbb0738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 13b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79662f93-64c0-451c-bf54-b5cfbe89e33e",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc366e-466c-42ed-8784-58bdb85f164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 20:31:53 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-17 20:32:01 llm_engine.py:275] # GPU blocks: 3631, # CPU blocks: 327\n",
      "INFO 01-17 20:32:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-17 20:32:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-17 20:32:06 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=codellama13b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c80a0-ac22-49ab-8083-7b31f3f6b8f9",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8205405-c60f-467f-b8f0-d88bac7bbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts(targets=[\"bible\"])\n",
    "generate_test(prompts, llm, \"codellama13b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488dfb-ef22-4378-8e97-e88f165732f8",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5fcfb9-f924-4400-b509-06a8144a8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama13b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e532c-b242-417c-9081-a9ef2675f6a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033ef5c2-23b9-490e-b6a5-b0c7d90ca7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_0_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_2_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_1_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_3_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_5_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_0_n1\": false,\n",
      "    \"date_fim_DateQuery_temp_0.2_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_greedy_1_n1\": false,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_3_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_5_n20\": 0.0\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_6_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_8_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_10_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_7_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_9_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_11_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": true,\n",
      "    \"date_fim_DateQuery_temp_0.2_8_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_10_n20\": 0.85,\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": true,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_9_n20\": 1.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_11_n20\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/singletok\", verbose), indent=4))\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/multitok\", verbose), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15650acb-eb97-4b45-8e85-29d39a59593c",
   "metadata": {},
   "source": [
    "## StarcoderBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4bc6fc-d048-446e-8149-5f9483a5d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-18 13:44:40 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-18 13:45:20 llm_engine.py:275] # GPU blocks: 131427, # CPU blocks: 13107\n",
      "INFO 01-18 13:45:22 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-18 13:45:22 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-18 13:45:25 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=starcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d474f-21a6-4fae-a3ae-55193d35edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts()\n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              1, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) \n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              20, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c02dd-6436-4000-ad8b-18f2da2a808b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f17cc41-0197-4f52-9059-859ba1ce2c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"transaction_fim_any_greedy_54_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_55_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"transaction_fim_any_greedy_60_n1\": [\n",
      "        true,\n",
      "        [\n",
      "            \"any\"\n",
      "        ],\n",
      "        \"any\"\n",
      "    ],\n",
      "    \"transaction_fim_void_greedy_61_n1\": [\n",
      "        false,\n",
      "        [\n",
      "            \"Promise<void>\"\n",
      "        ],\n",
      "        \"void\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "def pretty_print(dict, targets = None):\n",
    "    if targets:\n",
    "        dict = {k:v for k,v in dict.items() if all([t in k for t in targets])}\n",
    "    print(json.dumps(dict, indent=4))\n",
    "\n",
    "st = type_inf_eval(\"generations/starcoder/singletok\", verbose)\n",
    "mt = type_inf_eval(\"generations/starcoder/multitok\", verbose)\n",
    "\n",
    "targ = [\"greedy\", \"transaction\"]\n",
    "pretty_print(st, targ)\n",
    "pretty_print(mt, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d15df0-fc5b-4066-ba4a-e87c27f82ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
