{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d3d45-da94-40bc-973e-224f2ec65ac1",
   "metadata": {},
   "source": [
    "## Test type-inf dataset on LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67123b58-41f0-4771-91c0-e7e5a6b7a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# # set cuda visible device\n",
    "# !export CUDA_VISIBLE_DEVICES=0\n",
    "    \n",
    "!export TRANSFORMERS_CACHE=/work/arjunguha-research-group/franlucc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab860286-2a15-408a-a1af-8c1461ed48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/work/arjunguha-research-group\"\n",
    "codellama7b = f\"{root}/models/CodeLlama-7b-hf\"\n",
    "codellama13b = f\"{root}/models/CodeLlama-13b-hf\"\n",
    "starcoder = f\"{root}/arjun/models/starcoderbase\"\n",
    "\n",
    "from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n",
    "import torch\n",
    "\n",
    "def clear_gpu():\n",
    "    destroy_model_parallel()\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b90c37e-15b3-4a07-86a0-ab666a9d3d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'llm' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclear_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mclear_gpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclear_gpu\u001b[39m():\n\u001b[1;32m     10\u001b[0m     destroy_model_parallel()\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m llm\n\u001b[1;32m     12\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'llm' referenced before assignment"
     ]
    }
   ],
   "source": [
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc8c66-2f84-447f-9d2f-5b933ef8cd7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98eb32-9406-4abe-9a50-897ee6893f7d",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f01e838-9996-41bb-94e2-6245a22e6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 20:12:18 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-17 20:12:25 llm_engine.py:275] # GPU blocks: 7226, # CPU blocks: 512\n",
      "INFO 01-17 20:12:26 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-17 20:12:26 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-17 20:12:28 model_runner.py:547] Graph capturing finished in 2 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=codellama7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625802a2-eaa7-48e6-9f1a-89716f800089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_dataset import *\n",
    "\n",
    "codellama_fim = FimObj(\"<PRE> \", \" <SUF>\",\" <MID>\", \"<FILL>\") # codellama is finnicky with these spaces\n",
    "prompts = get_prompts(targets=[\"bible\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790c075-6eaa-473a-89f6-d1d665f059c2",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c41c608f-09c1-4dd8-99fd-d49a2bbd4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10.52it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.13it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama7b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0b2d9-eaaf-411b-81f1-5543fa17d8b3",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d525af-0d4d-4b8e-bc2e-59afecc6c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama7b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a97d0-a80b-499e-b6d9-dfcf22ee5061",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aa0a009-893f-482a-969f-c950620080ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_6_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_8_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_10_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_0_n1\": false,\n",
      "    \"bible_fim_unknown_greedy_7_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_1_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.2_9_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_11_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_0_n1\": false,\n",
      "    \"date_fim_DateQuery_temp_0.2_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_greedy_1_n1\": false,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_3_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_5_n20\": 0.0\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_12_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_14_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_16_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_13_n1\": false,\n",
      "    \"bible_fim_unknown_greedy_3_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_15_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.2_4_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_17_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_5_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": true,\n",
      "    \"date_fim_DateQuery_temp_0.2_8_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_10_n20\": 0.7,\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": true,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_9_n20\": 1.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_11_n20\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama7b/singletok\", verbose), indent=4))\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama7b/multitok\", verbose), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64942bf8-524a-4645-9e5e-6c66bdc873f3",
   "metadata": {},
   "source": [
    "### FIM sanity check\n",
    "since Codellama is getting all 0s, sanity check with a FIM example from CodeLlama tut: \n",
    "[codellama infill example](https://github.com/facebookresearch/codellama/blob/main/example_infilling.py)\n",
    "\n",
    "**EDIT** the problem was CodeLlama wants spaces in its special fim tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad664d2-7287-43a0-a250-da2f662475e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PRE> def remove_non_ascii(s: str) -> str:\\n    \"\"\"  <SUF>\\n    return result <MID>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove non-ASCII characters from a string.\n",
      "\n",
      "    Args:\n",
      "        s (str): The string to remove non-ASCII characters from.\n",
      "\n",
      "    Returns:\n",
      "        str: The string with non-ASCII characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cont = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result'''\n",
    "\n",
    "prompts = [placeholder_to_std_fmt(cont, codellama_fim)]\n",
    "print(prompts)\n",
    "output = llm.generate(prompts, greedy_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d57c5-c59a-4e1b-9ee4-d8750bbb0738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CodeLlama 13b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79662f93-64c0-451c-bf54-b5cfbe89e33e",
   "metadata": {},
   "source": [
    "CodeLlama 7b and 13b do code infilling using `<PRE> {prefix} <SUF>{suffix} <MID>`:\n",
    "```\n",
    "<PRE>def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"<SUF>\n",
    "    return result\n",
    "<MID>\n",
    "```\n",
    "or placeholder\n",
    "```\n",
    "def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL>\n",
    "    return result\n",
    "```\n",
    "then split and join with actual special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc366e-466c-42ed-8784-58bdb85f164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 20:31:53 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer='/work/arjunguha-research-group/models/CodeLlama-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-17 20:32:01 llm_engine.py:275] # GPU blocks: 3631, # CPU blocks: 327\n",
      "INFO 01-17 20:32:03 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-17 20:32:03 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-17 20:32:06 model_runner.py:547] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=codellama13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb368a5-f9b4-4edc-9440-1cd8c3ff3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_dataset import *\n",
    "\n",
    "codellama_fim = FimObj(\"<PRE> \", \" <SUF>\",\" <MID>\", \"<FILL>\")\n",
    "prompts = get_prompts(targets=[\"bible\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c80a0-ac22-49ab-8083-7b31f3f6b8f9",
   "metadata": {},
   "source": [
    "### Single token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8205405-c60f-467f-b8f0-d88bac7bbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama13b\", 1, {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488dfb-ef22-4378-8e97-e88f165732f8",
   "metadata": {},
   "source": [
    "### Multi token Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5fcfb9-f924-4400-b509-06a8144a8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_test(prompts, llm, \"codellama13b\", 50,  {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, codellama_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e532c-b242-417c-9081-a9ef2675f6a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033ef5c2-23b9-490e-b6a5-b0c7d90ca7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_0_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_2_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_1_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_3_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_5_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_0_n1\": false,\n",
      "    \"date_fim_DateQuery_temp_0.2_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_greedy_1_n1\": false,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_3_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_5_n20\": 0.0\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_6_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_8_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_10_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_7_n1\": false,\n",
      "    \"bible_fim_unknown_temp_0.2_9_n20\": 0.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_11_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": true,\n",
      "    \"date_fim_DateQuery_temp_0.2_8_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_10_n20\": 0.85,\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": true,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_9_n20\": 1.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_11_n20\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/singletok\", verbose), indent=4))\n",
    "print(json.dumps(type_inf_eval(\"generations/codellama13b/multitok\", verbose), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15650acb-eb97-4b45-8e85-29d39a59593c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## StarcoderBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4bc6fc-d048-446e-8149-5f9483a5d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 20:26:39 llm_engine.py:70] Initializing an LLM engine with config: model='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer='/work/arjunguha-research-group/arjun/models/starcoderbase', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-17 20:27:17 llm_engine.py:275] # GPU blocks: 131427, # CPU blocks: 13107\n",
      "INFO 01-17 20:27:19 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-17 20:27:19 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-17 20:27:22 model_runner.py:547] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=starcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045d474f-21a6-4fae-a3ae-55193d35edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  5.91it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.71it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from build_dataset import *\n",
    "\n",
    "prompts = get_prompts(targets=[\"bible\"])\n",
    "starcoder_fim = FimObj(\"<fim_prefix>\", \"<fim_suffix>\",\"<fim_middle>\", \"<FILL>\")\n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              1, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) \n",
    "generate_test(prompts, \n",
    "              llm, \n",
    "              \"starcoder\", \n",
    "              20, \n",
    "              {\"greedy\":greedy_params, \"temp_0.2\":sampling_eval, \"temp_0.8-top_p_0.95\": sampling_creative}, \n",
    "              starcoder_fim) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c02dd-6436-4000-ad8b-18f2da2a808b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f17cc41-0197-4f52-9059-859ba1ce2c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bible_fim_BookTitle_greedy_0_n1\": false,\n",
      "    \"bible_fim_BookTitle_temp_0.2_2_n20\": 0.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"bible_fim_unknown_greedy_1_n1\": true,\n",
      "    \"bible_fim_unknown_temp_0.2_3_n20\": 1.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_5_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_greedy_0_n1\": false,\n",
      "    \"date_fim_DateQuery_temp_0.2_2_n20\": 0.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_4_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_greedy_1_n1\": false,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_3_n20\": 0.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_5_n20\": 0.0\n",
      "}\n",
      "{\n",
      "    \"bible_fim_BookTitle_greedy_6_n1\": true,\n",
      "    \"bible_fim_BookTitle_temp_0.2_8_n20\": 1.0,\n",
      "    \"bible_fim_BookTitle_temp_0.8-top_p_0.95_10_n20\": 0.6000000000000001,\n",
      "    \"bible_fim_unknown_greedy_7_n1\": true,\n",
      "    \"bible_fim_unknown_temp_0.2_9_n20\": 1.0,\n",
      "    \"bible_fim_unknown_temp_0.8-top_p_0.95_11_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_greedy_6_n1\": true,\n",
      "    \"date_fim_DateQuery_temp_0.2_8_n20\": 1.0,\n",
      "    \"date_fim_DateQuery_temp_0.8-top_p_0.95_10_n20\": 0.85,\n",
      "    \"date_fim_ValidDateQuery_greedy_7_n1\": true,\n",
      "    \"date_fim_ValidDateQuery_temp_0.2_9_n20\": 1.0,\n",
      "    \"date_fim_ValidDateQuery_temp_0.8-top_p_0.95_11_n20\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "print(json.dumps(type_inf_eval(\"generations/starcoder/singletok\", verbose), indent=4))\n",
    "print(json.dumps(type_inf_eval(\"generations/starcoder/multitok\", verbose), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
