{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datasets\n",
    "import builtins\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "from typing import List\n",
    "import sys\n",
    "sys.path.append(\"/home/franlucc/projects/codetrace\")\n",
    "from codetrace.parsing_utils import placeholder_to_std_fmt, DEEPSEEK_FIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codellama completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['key', 'prefix', 'suffix', 'middle', 'correct', 'model', 'fim_type', 'fim_program', 'hexsha', 'generated_text'],\n",
       "     num_rows: 125507\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['key', 'prefix', 'suffix', 'middle', 'correct', 'model', 'fim_type', 'fim_program', 'hexsha'],\n",
       "     num_rows: 265879\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR=\"/home/franlucc/projects/codetrace\"\n",
    "ds = datasets.load_from_disk(f\"{DIR}/experiments/codellama_7b/py_completions\")\n",
    "ds_sc1_ = datasets.load_dataset(\"nuprl-staging/py_typeinf_fim\", split=\"train\")\n",
    "ds, ds_sc1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexshas = set(ds[\"fim_program\"])\n",
    "ds_sc1 = ds_sc1_.filter(lambda x: x[\"fim_program\"] in hexshas, num_proc=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n",
      "True     118953\n",
      "False      6554\n",
      "Name: count, dtype: int64 0.9477798051104719\n",
      "correct\n",
      "True     134511\n",
      "False    131368\n",
      "Name: count, dtype: int64 0.5059105833856754\n",
      "correct\n",
      "False    113228\n",
      "True      12279\n",
      "Name: count, dtype: int64 0.09783518050786012\n"
     ]
    }
   ],
   "source": [
    "df_sc1 = ds_sc1.to_pandas()\n",
    "df_sc1_ = ds_sc1_.to_pandas()\n",
    "df = ds.to_pandas()\n",
    "print(df_sc1[\"correct\"].value_counts(), df_sc1[\"correct\"].mean())\n",
    "print(df_sc1_[\"correct\"].value_counts(), df_sc1_[\"correct\"].mean())\n",
    "print(df[\"correct\"].value_counts(), df[\"correct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pattern(fim_type:str) -> str:\n",
    "    raw_pattern = r'from\\s+.+?\\s+import\\s+({fim_type})\\n|import\\s*.*?\\s+?({fim_type})\\n'\n",
    "    return raw_pattern.format(fim_type=fim_type)\n",
    "\n",
    "def is_imported(fim_type:str, fim_program:str) -> bool:\n",
    "    matches = re.search(get_pattern(fim_type), fim_program)\n",
    "    if not matches:\n",
    "        return False\n",
    "    # print([matches.group(i) for i in range(1, len(matches.groups()) + 1)])\n",
    "    return any(matches.group(i) == fim_type for i in range(1, len(matches.groups()) + 1))\n",
    "\n",
    "prog = '''from  oop import strop\n",
    "from oop.dilup import strop\n",
    "import Strofhsugf\n",
    "import igfhi from str\n",
    "\n",
    "from j import (\n",
    "    strop\n",
    ")\n",
    "\n",
    "from j.model import Integration, stroop\n",
    "\n",
    "'''\n",
    "is_imported(\"stroop\", prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x: {**x, \n",
    "                       \"is_imported_fim\" : is_imported(x[\"fim_type\"], x[\"fim_program\"]),\n",
    "                       \"is_builtin\": any([x[\"fim_type\"]==str(i) for i in dir(builtins)]),\n",
    "                       }, num_proc=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_imported_fim\n",
      "False    106442\n",
      "True      19065\n",
      "Name: count, dtype: int64\n",
      "is_imported_fim\n",
      "False    9998\n",
      "True     2281\n",
      "Name: count, dtype: int64\n",
      "0.09783518050786012\n"
     ]
    }
   ],
   "source": [
    "df = ds.to_pandas()\n",
    "df[\"imported_fim_or_builtin\"] = df[\"is_builtin\"] | df[\"is_imported_fim\"]\n",
    "df[\"imported_fim_and_builtin\"] = df[\"is_builtin\"] & df[\"is_imported_fim\"]\n",
    "df_correct = df[df[\"correct\"]]\n",
    "df_incorrect = df[~df[\"correct\"]]\n",
    "print(df[\"is_imported_fim\"].value_counts())\n",
    "print((df_correct[\"is_imported_fim\"]).value_counts())\n",
    "print(df[\"correct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18576431305480903\n",
      "0.7217200097727828\n",
      "is_imported_fim\n",
      "False    9998\n",
      "True     2281\n",
      "Name: count, dtype: int64\n",
      "is_builtin\n",
      "True     8862\n",
      "False    3417\n",
      "Name: count, dtype: int64\n",
      "imported_fim_or_builtin\n",
      "True     11138\n",
      "False     1141\n",
      "Name: count, dtype: int64\n",
      "imported_fim_and_builtin\n",
      "False    12274\n",
      "True         5\n",
      "Name: count, dtype: int64\n",
      "0.20479439755790985\n",
      "0.7956545160711079\n",
      "0.9070771235442625\n",
      "fim_type\n",
      "bool    3\n",
      "int     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def ignore_types(d, types):\n",
    "    for t in types:\n",
    "        d = d[d[\"fim_type\"] != str(t)]\n",
    "    return d.reset_index()\n",
    " \n",
    "df_corr_imported = df_correct[df_correct[\"is_imported_fim\"]].reset_index()\n",
    "df_incorr_imported = df_incorrect[df_incorrect[\"is_imported_fim\"]].reset_index()\n",
    "# df_ic = ignore_types(df_, dir(builtins))\n",
    "# k=2\n",
    "# print(df_ic[\"fim_program\"][k], df_ic[\"fim_type\"][k], df_ic[\"is_imported_fim\"][k])\n",
    "\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(df_ic[\"fim_type\"].value_counts())\n",
    "\n",
    "print(df_correct[\"is_imported_fim\"].mean())\n",
    "print(df_correct[\"is_builtin\"].mean())\n",
    "\n",
    "print(df_correct[\"is_imported_fim\"].value_counts())\n",
    "print(df_correct[\"is_builtin\"].value_counts())\n",
    "print(df_correct[\"imported_fim_or_builtin\"].value_counts())\n",
    "print(df_correct[\"imported_fim_and_builtin\"].value_counts())\n",
    "\n",
    "print(df_correct[\"is_imported_fim\"].sum() / df_correct[\"imported_fim_or_builtin\"].sum())\n",
    "print(df_correct[\"is_builtin\"].sum() / df_correct[\"imported_fim_or_builtin\"].sum())\n",
    "print(df_correct[\"imported_fim_or_builtin\"].mean())\n",
    "\n",
    "print(df_correct[df_correct[\"imported_fim_and_builtin\"]][\"fim_type\"].value_counts())\n",
    "item = df_correct[df_correct[\"imported_fim_and_builtin\"]].reset_index()\n",
    "# print(item[\"fim_program\"][0], item[\"fim_type\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts = pd.read_csv(f\"{DIR}/experiments/codellama_7b/nov7_test_all_v2_incorrect_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nan, '\\n', 'float')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=67\n",
    "muts[\"generated_text\"][k],muts[\"mutated_generated_text\"][k],muts[\"fim_type\"][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mutated_generated_text\n",
       "\\n    1353\n",
       "\"\"      21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muts[\"mutated_generated_text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated_text\n",
       "            109052\n",
       "\"\"            2164\n",
       "#              314\n",
       "\"\"\"            229\n",
       "~~~~~~~~       168\n",
       "             ...  \n",
       "any              1\n",
       "subset           1\n",
       "tp               1\n",
       "sound            1\n",
       "Se               1\n",
       "Name: count, Length: 211, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incorrect[\"generated_text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09783518050786012, 125507)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"correct\"].mean(), df[\"correct\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['key', 'prefix', 'suffix', 'middle', 'correct', 'model', 'fim_type', 'fim_program', 'hexsha'],\n",
       "        num_rows: 265879\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_data = datasets.load_dataset(\"nuprl-staging/py_typeinf_fim\")\n",
    "py_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-08 13:36:23 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-08 13:36:23 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-08 13:36:23 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/ssd/franlucc/models/deepseek-coder-6.7b-base', speculative_config=None, tokenizer='/mnt/ssd/franlucc/models/deepseek-coder-6.7b-base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/ssd/franlucc/models/deepseek-coder-6.7b-base, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-08 13:36:24 model_runner.py:720] Starting to load model /mnt/ssd/franlucc/models/deepseek-coder-6.7b-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:29<00:00, 15.85s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:29<00:00, 14.61s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-08 13:36:54 model_runner.py:732] Loading model weights took 12.5708 GB\n",
      "INFO 11-08 13:36:54 gpu_executor.py:102] # GPU blocks: 7446, # CPU blocks: 512\n",
      "INFO 11-08 13:36:56 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-08 13:36:56 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-08 13:37:06 model_runner.py:1225] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"/mnt/ssd/franlucc/models/deepseek-coder-6.7b-base\", device=\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(llm: LLM, prompts:List[str])->List[str]:\n",
    "    params = SamplingParams(max_tokens=1, temperature=0, n=1)\n",
    "    outputs = llm.generate(prompts, sampling_params=params)\n",
    "    res =[]\n",
    "    for req_output in outputs:\n",
    "        for output in req_output.outputs:\n",
    "            res.append(output.text)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 24.20it/s, est. speed input: 364.56 toks/s, output: 24.28 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['int']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = '''\n",
    "def factorial(n: <FILL>):\n",
    "    pass\n",
    "'''.strip()\n",
    "\n",
    "generate(model, placeholder_to_std_fmt(prompt, DEEPSEEK_FIM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
